{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQcCQ7kyNX5E"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "import tensorflow_probability as tfp\n",
    "from scipy.ndimage import gaussian_filter1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfsyNoi-HSo1"
   },
   "source": [
    "### cifar experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to create TF datasets\n",
    "\n",
    "def normalize_img(image, label):\n",
    "    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "    image = tf.cast(image, tf.float32) / 255.\n",
    "    # image = tf.reshape(image, (-1,))\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def make_ds(ds):\n",
    "    ds = ds.map(\n",
    "        normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.cache()\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.shuffle(int(1e5))\n",
    "    ds = ds.batch(128)\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset, iterate over it and train, return model and metrics\n",
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        print(f\"\\nRun training with params {self.params}\")\n",
    "        \n",
    "        # create CIFAR dataset and get iterators\n",
    "        ds_train, ds_test = out = tfds.load(\n",
    "            'cifar10',\n",
    "            split=['train', 'test'],\n",
    "            shuffle_files=True,\n",
    "            as_supervised=True,\n",
    "            with_info=False,\n",
    "        )\n",
    "        ds_train = make_ds(ds_train)\n",
    "        ds_test = make_ds(ds_test)\n",
    "        self.iter_train = iter(ds_train)\n",
    "        self.iter_test = iter(ds_test)\n",
    "\n",
    "        # define model\n",
    "        self.model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=params['input_shape']),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.params['n_classes'], activation='softmax')])\n",
    "        print(self.model.summary())\n",
    "\n",
    "        # define optimizer\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.params['lr'])\n",
    "        \n",
    "        # maintain history\n",
    "        self.history = []\n",
    "\n",
    "\n",
    "    # define loss function\n",
    "    # computes loss given a model and X, Y\n",
    "    @tf.function\n",
    "    def loss_fn(self, X, Y):\n",
    "        Y_hat = self.model(X) \n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.argmax(Y_hat, axis=1) == Y, tf.float32)) * 100.\n",
    "\n",
    "        # Do PGD\n",
    "        step_size = 0.1\n",
    "\n",
    "        if self.params['version'] == 'original':\n",
    "            # compute gradient of cross entropy loss wrt X and take a step in -ve direction\n",
    "            # this would try to find a point in the neighborhood of X that minimizes cross entropy\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(X)\n",
    "                Y_hat = self.model(X)\n",
    "                loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "            grads = tape.gradient(loss, X)\n",
    "            grads_norm = tf.norm(tf.reshape(grads, (128, -1)), axis=1)\n",
    "            grads = -step_size * grads / grads_norm[:, None, None, None]\n",
    "            X_perturbed = tf.clip_by_value(X + grads, 0.0, 1.0)\n",
    "            # compute -ve entropy at this new point\n",
    "            Y_hat = self.model(X_perturbed)  \n",
    "            entropy = -1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat), axis=1)\n",
    "            loss_adv = -1.0 * tf.reduce_mean(entropy)\n",
    "            # loss_adv = tf.keras.losses.CategoricalCrossentropy(from_logits=False)(y_true=tf.ones_like(Y_hat) / params['n_classes'], y_pred=Y_hat)\n",
    "            return loss + self.params['lambda'] * loss_adv, loss_adv, accuracy\n",
    "\n",
    "        elif self.params['version'] == 'entropy':\n",
    "            # compute grad of entropy wrt X and take a step in negative direction\n",
    "            # this would find a point in the neighborhood of X that would minimize entropy\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(X)\n",
    "                Y_hat = self.model(X)\n",
    "                entropy = -1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat), axis=1)\n",
    "            grads = tape.gradient(entropy, X)\n",
    "            grads_norm = tf.norm(tf.reshape(grads, (128, -1)), axis=1)\n",
    "            grads = -step_size * grads / grads_norm[:, None, None, None]      \n",
    "            X_perturbed = tf.clip_by_value(X + grads, 0.0, 1.0)\n",
    "            # compute entropy at this new point and multiply it by -1, since we want to maximize entropy\n",
    "            Y_hat = self.model(X_perturbed)\n",
    "            entropy = -1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat), axis=1)\n",
    "            loss_adv = -1.0 * tf.reduce_mean(entropy)\n",
    "            return loss + self.params['lambda'] * loss_adv, loss_adv, accuracy\n",
    "\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "\n",
    "    # define step function\n",
    "    # computes gradients and applies them\n",
    "    @tf.function\n",
    "    def step_fn(self, X_train, Y_train):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss, loss_adv, accuracy = self.loss_fn(X_train, Y_train)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss, loss_adv, accuracy\n",
    "    \n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def eval_ood_helper(self, X_real, X_fake):\n",
    "        Y_hat_real = self.model(X_real)\n",
    "        entropy_real = -1.0 * tf.reduce_mean(Y_hat_real * tf.math.log(Y_hat_real), axis=1)\n",
    "        Y_hat_fake = self.model(X_fake)\n",
    "        entropy_fake = -1.0 * tf.reduce_mean(Y_hat_fake * tf.math.log(Y_hat_fake), axis=1)\n",
    "        return tf.concat([entropy_real, entropy_fake], axis=0)\n",
    "\n",
    "    def eval_ood(self, X_real, X_fake):\n",
    "        logits = trainer.eval_ood_helper(X_real, X_fake).numpy()\n",
    "        labels = np.concatenate([np.zeros(128), np.ones(128)])\n",
    "        auc = roc_auc_score(labels, logits)\n",
    "        return auc\n",
    "\n",
    "    @tf.function\n",
    "    def get_calibration_metrics(self, X, Y):\n",
    "        logits = tf.math.log(self.model(X))\n",
    "        brier = tf.reduce_mean(tfp.stats.brier_score(labels=Y, logits=logits))\n",
    "        ece = tfp.stats.expected_calibration_error(num_bins=20, logits=logits, labels_true=Y)\n",
    "        nll = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(y_true=Y, y_pred=logits)\n",
    "        return brier, ece, nll\n",
    "\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        # loop over data n_iters times\n",
    "        for t in tqdm.trange(self.params['n_iters']):\n",
    "            X_train, Y_train = next(self.iter_train)\n",
    "            train_loss, train_loss_adv, train_acc = self.step_fn(X_train, Y_train)\n",
    "            if t % 10 == 0:\n",
    "                X_test, Y_test = next(self.iter_test)\n",
    "                test_loss, test_loss_adv, test_acc = self.loss_fn(X_test, Y_test)\n",
    "                self.history.append((train_loss.numpy(), test_loss.numpy(), train_acc.numpy(), test_acc.numpy(),\n",
    "                                    train_loss_adv.numpy(), test_loss_adv.numpy()))\n",
    "        \n",
    "        self.history = np.array(self.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting utils\n",
    "\n",
    "def plot_training_metrics(trainer_vec):\n",
    "    c_vec = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for idx, (c, trainer) in enumerate(zip(c_vec, trainer_vec)):\n",
    "        name = 'baseline' if idx == 0 else 'ours'\n",
    "        plt.plot(gaussian_filter1d(trainer.history[:, 2], 100), '-o', c=c, label='%s - train' % name, markevery=1000, markersize=10)\n",
    "        plt.plot(gaussian_filter1d(trainer.history[:, 3], 100), '-^', c=c, label='%s - val' % name, markevery=1000, markersize=10)\n",
    "    plt.grid()\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_OOD_metrics(results):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for (j, title) in enumerate(['Accuracy $(\\\\uparrow)$', 'OOD AUC $(\\\\uparrow)$', 'Brier $(\\\\downarrow)$', 'ECE  $(\\\\downarrow)$', 'NLL  $(\\\\downarrow)$']):\n",
    "        plt.subplot(2, 3, j + 1)\n",
    "        plt.title(title)\n",
    "        for idx in range(len(trainer_vec)):\n",
    "            x = np.arange(len(corruption_type_list))\n",
    "            y = [results[(ctype, idx)][j] for ctype in corruption_type_list]\n",
    "            width = 0.25\n",
    "            offset = width\n",
    "            plt.bar(x + width * (idx + 1) - offset, y, width=width, label='%s' % ('ours' if idx else 'baseline'))\n",
    "        plt.xticks(np.arange(len(corruption_type_list)), corruption_type_list, rotation=90)\n",
    "        plt.grid()\n",
    "        if j == 0:\n",
    "            plt.legend(fontsize=12, loc='lower right')\n",
    "        if j == 0:\n",
    "            plt.ylim([0.4, 0.75])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline\n",
    "params = {\n",
    "    'input_shape': (32, 32, 3),\n",
    "    'n_classes': 10,\n",
    "    'lambda': 0.0,\n",
    "    'lr': 3e-4,\n",
    "    'n_iters': 10000,\n",
    "    'version': 'original'\n",
    "}\n",
    "\n",
    "baseline_trainer = Trainer(params)\n",
    "baseline_trainer.train()\n",
    "\n",
    "# our method\n",
    "params = {\n",
    "    'input_shape': (32, 32, 3),\n",
    "    'n_classes': 10,\n",
    "    'lambda': 1.0,\n",
    "    'lr': 3e-4,\n",
    "    'n_iters': 10000,\n",
    "    'version': 'original'\n",
    "}\n",
    "\n",
    "\n",
    "our_trainer = Trainer(params)\n",
    "our_trainer.train()\n",
    "\n",
    "# plot\n",
    "trainer_vec = [baseline_trainer, our_trainer]\n",
    "plot_training_metrics(trainer_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute OOD metrics\n",
    "results = {}\n",
    "corruption_type_list = ['brightness_1', 'elastic_5', 'fog_5', 'frost_5', 'frosted_glass_blur_5']\n",
    "\n",
    "for idx, trainer in enumerate(trainer_vec):\n",
    "    print(f\"Trainer {idx + 1}\")\n",
    "    for corruption_type in corruption_type_list:\n",
    "        print(corruption_type)\n",
    "        \n",
    "        # load corrupted dataset\n",
    "        (ds_corrupted,) = tfds.load(\n",
    "            'cifar10_corrupted/%s' % corruption_type,\n",
    "            split=['test'],\n",
    "            shuffle_files=True,\n",
    "            as_supervised=True,\n",
    "            with_info=False,\n",
    "        )\n",
    "        \n",
    "        # make dataset and get iterator\n",
    "        ds_corrupted = make_ds(ds_corrupted)\n",
    "        iter_corrupted = iter(ds_corrupted)\n",
    "\n",
    "        # record metrics\n",
    "        acc_vec = []\n",
    "        auc_vec = []\n",
    "        brier_vec = []\n",
    "        ece_vec = []\n",
    "        nll_vec = []\n",
    "        \n",
    "        # average over a 100 batches\n",
    "        for _ in tqdm.trange(100):\n",
    "            X_test, Y_test = next(trainer.iter_test)\n",
    "            _, _, test_acc = trainer.loss_fn(X_test, Y_test)\n",
    "            X_corrupted, Y_corrupted = next(iter_corrupted)\n",
    "            test_auc = trainer.eval_ood(X_test, X_corrupted)\n",
    "            brier, ece, nll = trainer.get_calibration_metrics(X_corrupted, Y_corrupted)\n",
    "\n",
    "            acc_vec.append(test_acc.numpy())\n",
    "            auc_vec.append(test_auc)\n",
    "            brier_vec.append(brier.numpy())\n",
    "            ece_vec.append(ece.numpy())\n",
    "            nll_vec.append(nll.numpy())\n",
    "        \n",
    "        results[(corruption_type, idx)] = (np.mean(acc_vec), np.mean(auc_vec), np.mean(brier_vec), np.mean(ece_vec), np.mean(nll_vec))\n",
    "\n",
    "plot_OOD_metrics(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FVzDc6jHUi1"
   },
   "source": [
    "### mnist experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 211826,
     "status": "ok",
     "timestamp": 1628699671992,
     "user": {
      "displayName": "Benjamin Eysenbach",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEfAvIjDMhPruzTN48aaHvZ9wT0yOGBM5MVcLGtg=s64",
      "userId": "13423046094271950891"
     },
     "user_tz": 240
    },
    "id": "dT0oGM2VNo9Z",
    "outputId": "b0a16c1a-937b-4b3a-91f7-7967f37025b0"
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "corruption_type_list = ['shot_noise', 'impulse_noise', 'rotate', 'canny_edges']\n",
    "for corruption_type in corruption_type_list:\n",
    "  (ds_train, ds_test), ds_info = tfds.load(\n",
    "      'mnist_corrupted/identity',\n",
    "      split=['train', 'test'],\n",
    "      shuffle_files=True,\n",
    "      as_supervised=True,\n",
    "      with_info=True,\n",
    "  )\n",
    "\n",
    "  (_, ds_corrupted), ds_info = tfds.load(\n",
    "      'mnist_corrupted/%s' % corruption_type,\n",
    "      split=['train', 'test'],\n",
    "      shuffle_files=True,\n",
    "      as_supervised=True,\n",
    "      with_info=True,\n",
    "  )\n",
    "\n",
    "  def normalize_img(image, label):\n",
    "    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "    image = tf.cast(image, tf.float32) / 255.\n",
    "    image = tf.reshape(image, (-1,))\n",
    "    return image, label\n",
    "\n",
    "  def make_ds(ds):\n",
    "    ds = ds.map(\n",
    "      normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.cache().repeat()\n",
    "    ds = ds.shuffle(ds_info.splits['train'].num_examples)\n",
    "    ds = ds.batch(128)\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "  ds_train = make_ds(ds_train)\n",
    "  iter_train = iter(ds_train)\n",
    "\n",
    "  ds_test = make_ds(ds_test)\n",
    "  iter_test = iter(ds_test)\n",
    "\n",
    "  ds_corrupted = make_ds(ds_corrupted)\n",
    "  iter_corrupted = iter(ds_corrupted)\n",
    "\n",
    "  for L in [0.0, 100.0]:\n",
    "    print(corruption_type, L)\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Dense(128, activation='relu'),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "    @tf.function\n",
    "    def loss_fn(X, Y):\n",
    "      Y_hat = model(X)\n",
    "      loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "      accuracy = tf.reduce_mean(tf.cast(tf.argmax(Y_hat, axis=1) == Y, tf.float32))\n",
    "\n",
    "      ### Original version\n",
    "      version = 'original'\n",
    "      if version == 'original':\n",
    "        with tf.GradientTape() as tape:\n",
    "          tape.watch(X)\n",
    "          Y_hat = model(X)\n",
    "          loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "        grads = tape.gradient(loss, X)\n",
    "        grads = -0.001 * grads / tf.norm(grads, axis=1)[:, None]\n",
    "        X = tf.clip_by_value(X + grads, 0.0, 1.0)\n",
    "\n",
    "        Y_hat = model(X)  \n",
    "        loss2 = tf.keras.losses.CategoricalCrossentropy(from_logits=False)(y_true=tf.ones_like(Y_hat) / 10.0, y_pred=Y_hat)\n",
    "        return loss + L * loss2, loss2, accuracy\n",
    "      elif version == 'entropy':\n",
    "        with tf.GradientTape() as tape:\n",
    "          tape.watch(X)\n",
    "          Y_hat = model(X)\n",
    "          entropy = -1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat), axis=1)\n",
    "        grads = tape.gradient(entropy, X)\n",
    "        grads = -0.001 * grads / tf.norm(grads, axis=1)[:, None]\n",
    "        X = tf.clip_by_value(X + grads, 0.0, 1.0)\n",
    "\n",
    "        Y_hat = model(X)\n",
    "        entropy = -1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat), axis=1)\n",
    "        loss2 = -1.0 * tf.reduce_mean(entropy)\n",
    "        return loss + L * loss2, loss2, accuracy, X\n",
    "      else:\n",
    "        raise ValueError\n",
    "\n",
    "    @tf.function\n",
    "    def step_fn(X_train, Y_train):\n",
    "      with tf.GradientTape() as tape:\n",
    "        loss, loss2, accuracy, X = loss_fn(X_train, Y_train)\n",
    "      grads = tape.gradient(loss, model.trainable_variables)\n",
    "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "      return loss, loss2, accuracy, X\n",
    "\n",
    "    @tf.function\n",
    "    def eval_ood_helper(X_real, X_fake):\n",
    "      Y_hat_real = model(X_real)\n",
    "      entropy_real = -1.0 * tf.reduce_mean(Y_hat_real * tf.math.log(Y_hat_real), axis=1)\n",
    "      Y_hat_fake = model(X_fake)\n",
    "      entropy_fake = -1.0 * tf.reduce_mean(Y_hat_fake * tf.math.log(Y_hat_fake), axis=1)\n",
    "      return tf.concat([entropy_real, entropy_fake], axis=0)\n",
    "\n",
    "    def eval_ood(X_real, X_fake):\n",
    "      logits = eval_ood_helper(X_real, X_fake).numpy()\n",
    "      labels = np.concatenate([np.zeros(128), np.ones(128)])\n",
    "      auc = roc_auc_score(labels, logits)\n",
    "      return auc\n",
    "\n",
    "    history = []\n",
    "    for t in tqdm.trange(5000):\n",
    "      X_train, Y_train = next(iter_train)\n",
    "      train_loss, train_loss2, train_acc, X = step_fn(X_train, Y_train)\n",
    "      if t % 10 == 0:\n",
    "        X_test, Y_test = next(iter_test)\n",
    "        test_loss, test_loss2, test_acc, _ = loss_fn(X_test, Y_test)\n",
    "        X_corrupted, _ = next(iter_corrupted)\n",
    "        test_auc = eval_ood(X_test, X_corrupted)\n",
    "      history.append((train_loss.numpy(), test_loss.numpy(), train_acc.numpy(), test_acc.numpy(),\n",
    "                      train_loss2.numpy(), test_loss2.numpy(), test_auc))\n",
    "      if t % 100 == 0:\n",
    "        print(test_auc)\n",
    "    history = np.array(history)\n",
    "    test_acc = np.mean(history[-500:, 3])\n",
    "    test_auc = np.mean(history[-500:, 6])\n",
    "    results[(corruption_type, L)] = (test_acc, test_auc)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(221)\n",
    "    plt.title('Loss')\n",
    "    plt.plot(history[:, 0], label='train')\n",
    "    plt.plot(history[:, 1], label='val')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(222)\n",
    "    plt.title('Accuracy')\n",
    "    plt.plot(history[:, 2], label='train')\n",
    "    plt.plot(history[:, 3], label='val')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(223)\n",
    "    plt.title('Loss2')\n",
    "    plt.plot(history[:, 4], label='train')\n",
    "    plt.plot(history[:, 5], label='val')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(224)\n",
    "    plt.title('AUC')\n",
    "    plt.plot(history[:, 6])\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 228
    },
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1628699713505,
     "user": {
      "displayName": "Benjamin Eysenbach",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEfAvIjDMhPruzTN48aaHvZ9wT0yOGBM5MVcLGtg=s64",
      "userId": "13423046094271950891"
     },
     "user_tz": 240
    },
    "id": "kUsUx9bBN6cP",
    "outputId": "2b6efcc9-a288-4bbd-8cac-c36469b1acab"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "x = np.arange(len(corruption_type_list))\n",
    "for (index, title) in enumerate(['Accuracy', 'OOD AUC']):\n",
    "  plt.subplot(1, 2, index + 1)\n",
    "  plt.title(title)\n",
    "  for L in [0.0, 100.0]:\n",
    "    y = []\n",
    "    for corruption_type in corruption_type_list:\n",
    "      y.append(results[(corruption_type, L)][index])\n",
    "    plt.bar(x + 0.3 * (1 if L else 0) - 0.15, y, width=0.3, label='L = %s (%s)' % (L, 'ours' if L else 'baseline'))\n",
    "  plt.xticks(x, corruption_type_list)\n",
    "  plt.grid()\n",
    "  plt.legend(fontsize=12, loc='lower right')\n",
    "  if index == 0:\n",
    "    plt.ylim([0.9, 1.0])\n",
    "  # else:\n",
    "  #   plt.ylim([0.6, 1.0])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "name": "predictive entropy.ipynb",
   "provenance": [
    {
     "file_id": "1JQbpV3_b5nJeSwR-vLJ-t9xoCu33COGH",
     "timestamp": 1630008033655
    },
    {
     "file_id": "1l0iWRfgfMvCsvz6Bqu-RcHKkeHEF48qb",
     "timestamp": 1628698030421
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
