{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PQcCQ7kyNX5E"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import tqdm\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow_probability as tfp\n",
    "from scipy.ndimage import gaussian_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfsyNoi-HSo1"
   },
   "source": [
    "### cifar experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to create TF datasets\n",
    "\n",
    "def normalize_img(image, label):\n",
    "    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "    image = tf.cast(image, tf.float32) / 255.\n",
    "    # image = tf.reshape(image, (-1,))\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def make_ds(ds, augment=False):\n",
    "    ds = ds.map(\n",
    "        normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.cache()\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.shuffle(int(1e5))\n",
    "    ds = ds.batch(128)\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-3\n",
    "\n",
    "@tf.function\n",
    "def baseline(X, Y, model, training, **params):\n",
    "    Y_hat = model(X, training=training) \n",
    "    ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.argmax(Y_hat, axis=1) == Y, tf.float32)) * 100.\n",
    "    entropy_on_original_point = tf.reduce_mean(-1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat + EPS), axis=1))\n",
    "    return ce_loss, ce_loss, 0., accuracy, entropy_on_original_point\n",
    "        \n",
    "@tf.function\n",
    "def min_max_cent(X, Y, model, training, **params):\n",
    "    # compute gradient of cross entropy loss wrt X and take a step in +ve direction\n",
    "    # this would try to find a point in the neighborhood of X that maximizes cross entropy\n",
    "    Y_hat = model(X, training=training) \n",
    "    ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.argmax(Y_hat, axis=1) == Y, tf.float32)) * 100.\n",
    "    entropy_on_original_point = tf.reduce_mean(-1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat + EPS), axis=1))    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(X)\n",
    "        Y_hat = model(X, training=training)\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "    grads = tape.gradient(loss, X)\n",
    "    grads_norm = tf.norm(tf.reshape(grads, (128, -1)), axis=1)\n",
    "    grads = params['step_size'] * grads / grads_norm[:, None, None, None]\n",
    "    X_perturbed = tf.clip_by_value(X + grads, 0.0, 1.0)\n",
    "    # compute cross entropy at this new point\n",
    "    Y_hat = model(X_perturbed, training=training)\n",
    "    loss_adv = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "    return ce_loss + params['lambda'] * loss_adv, ce_loss, loss_adv, accuracy, entropy_on_original_point\n",
    "\n",
    "@tf.function\n",
    "def max_min_ent(X, Y, model, training, **params):\n",
    "    # compute grad of entropy wrt X and take a step in negative direction\n",
    "    # this would find a point in the neighborhood of X that would minimize entropy\n",
    "    Y_hat = model(X, training=training) \n",
    "    ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.argmax(Y_hat, axis=1) == Y, tf.float32)) * 100.\n",
    "    entropy_on_original_point = tf.reduce_mean(-1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat + EPS), axis=1))    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(X)\n",
    "        Y_hat = model(X, training=training)\n",
    "        exp_neg_entropy = tf.exp(tf.reduce_mean(Y_hat * tf.math.log(Y_hat + EPS), axis=1))\n",
    "    grads = tape.gradient(exp_neg_entropy, X)\n",
    "    grads_norm = tf.norm(tf.reshape(grads, (128, -1)), axis=1)\n",
    "    grads = params['step_size'] * grads / grads_norm[:, None, None, None]      \n",
    "    X_perturbed = tf.clip_by_value(X + grads, 0.0, 1.0)\n",
    "    # compute entropy at this new point and multiply it by -1 (raise to exp. for better grads), since we want to maximize entropy\n",
    "    Y_hat = model(X_perturbed, training=training)\n",
    "    entropy = -1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat + EPS), axis=1)\n",
    "    loss_adv = tf.reduce_mean(tf.exp(-1.0 * entropy))\n",
    "    return ce_loss + params['lambda'] * loss_adv, ce_loss, loss_adv, accuracy, entropy_on_original_point\n",
    "\n",
    "@tf.function\n",
    "def min_max_KL_unif(X, Y, model, training, **params):\n",
    "    # compute grad of KL(unif, p_\\theta) wrt X and take a step in +ve direction\n",
    "    # this would find a point in the neighborhood of X that would maximize KL\n",
    "    Y_hat = model(X, training=training) \n",
    "    ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.argmax(Y_hat, axis=1) == Y, tf.float32)) * 100.\n",
    "    entropy_on_original_point = tf.reduce_mean(-1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat + 1e-3), axis=1)) \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(X)\n",
    "        Y_hat = model(X, training=training)\n",
    "        KL_unif = tf.keras.losses.CategoricalCrossentropy(from_logits=False)(\n",
    "            y_true=tf.ones_like(Y_hat) / tf.cast(params['n_classes'], tf.float32), y_pred=Y_hat)\n",
    "    grads = tape.gradient(KL_unif, X)\n",
    "    grads_norm = tf.norm(tf.reshape(grads, (128, -1)), axis=1)\n",
    "    grads = params['step_size'] * grads / grads_norm[:, None, None, None]      \n",
    "    X_perturbed = tf.clip_by_value(X + grads, 0.0, 1.0)\n",
    "    # compute entropy at this new point and multiply it by -1, since we want to maximize entropy\n",
    "    Y_hat = model(X_perturbed, training=training)\n",
    "    KL_unif = tf.keras.losses.CategoricalCrossentropy(from_logits=False)(\n",
    "        y_true=tf.ones_like(Y_hat) / tf.cast(params['n_classes'], tf.float32), y_pred=Y_hat)\n",
    "    loss_adv = tf.reduce_mean(KL_unif)\n",
    "    return ce_loss + params['lambda'] * loss_adv, ce_loss, loss_adv, accuracy, entropy_on_original_point\n",
    "\n",
    "@tf.function\n",
    "def label_smoothing(X, Y, model, training, **params):\n",
    "    Y_hat = model(X, training=training)\n",
    "    ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.argmax(Y_hat, axis=1) == Y, tf.float32)) * 100.\n",
    "    entropy_on_original_point = tf.reduce_mean(-1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat + 1e-3), axis=1)) \n",
    "    Y = tf.one_hot(Y, params['n_classes'])\n",
    "    Y_noisy = Y * (1 - params['label-smoothing-factor']) \n",
    "    Y_noisy += (params['label-smoothing-factor'] / tf.cast(params['n_classes'], tf.float32))\n",
    "    noisy_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)(y_true=Y_noisy, y_pred=Y_hat)\n",
    "    return noisy_loss, ce_loss, 0., accuracy, entropy_on_original_point\n",
    "\n",
    "@tf.function\n",
    "def get_calibration_metrics(X, Y, model, tau):\n",
    "    logits = tf.math.log(model(X, training=False) + EPS) * tf.repeat(tau, X.shape[0], axis=0)\n",
    "    brier = tf.reduce_mean(tfp.stats.brier_score(labels=Y, logits=logits))\n",
    "    ece = tfp.stats.expected_calibration_error(num_bins=20, logits=logits, labels_true=Y)\n",
    "    nll = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(y_true=Y, y_pred=logits)\n",
    "    return brier, ece, nll\n",
    "    \n",
    "    \n",
    "@tf.function\n",
    "def eval_ood_helper(model, X_real, X_fake, tau):\n",
    "    Y_hat_real = tf.nn.softmax(tf.math.log(model(X_real, training=False) + EPS) * tf.repeat(tau, X_real.shape[0], axis=0))\n",
    "    entropy_real = -1.0 * tf.reduce_mean(Y_hat_real * tf.math.log(Y_hat_real + EPS), axis=1)\n",
    "    Y_hat_fake = tf.nn.softmax(tf.math.log(model(X_fake, training=False) + EPS) * tf.repeat(tau, X_fake.shape[0], axis=0))\n",
    "    entropy_fake = -1.0 * tf.reduce_mean(Y_hat_fake * tf.math.log(Y_hat_fake + EPS), axis=1)\n",
    "    return tf.concat([entropy_real, entropy_fake], axis=0)\n",
    "\n",
    "\n",
    "def conv_to_tensors(p):\n",
    "    return {k: tf.constant(v) if type(v) != dict else conv_to_tensors(v) for k, v in p.items()}\n",
    "\n",
    "# define loss function\n",
    "# computes loss given a model and X, Y\n",
    "@ tf.function\n",
    "def loss_fn(X, Y, model, training, **params):\n",
    "\n",
    "    if params['version'] == 'baseline':\n",
    "        return baseline(X, Y, model, training, **params)\n",
    "    elif params['version'] == 'min-max-cent':\n",
    "        return min_max_cent(X, Y, model, training, **params)\n",
    "    elif params['version'] == 'max-min-ent':\n",
    "        return max_min_ent(X, Y, model, training, **params)\n",
    "    elif params['version'] == 'min-max-KL-unif': \n",
    "        return min_max_KL_unif(X, Y, model, training, **params)\n",
    "    elif params['version'] == 'label-smoothing':\n",
    "        return label_smoothing(X, Y, model, training, **params)\n",
    "    else:\n",
    "        return 0., 0., 0., 0., 0.\n",
    "\n",
    "# define step function\n",
    "# computes gradients and applies them\n",
    "@tf.function\n",
    "def step_fn(X, Y, model, optimizer, **params):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, cent_loss, loss_adv, accuracy, predent = loss_fn(X, Y, model, True, **params)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss, cent_loss, loss_adv, accuracy, predent\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset, iterate over it and train, return model and metrics\n",
    "\n",
    "EPS = 1e-3\n",
    "    \n",
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        print(f\"\\nRun training with params {self.params}\")\n",
    "    \n",
    "        ds_train, ds_test = out = tfds.load(\n",
    "            'cifar10',\n",
    "            split=['train', 'test'],\n",
    "            data_dir='/amrith/tensorflow_datasets',\n",
    "            shuffle_files=True,\n",
    "            as_supervised=True,\n",
    "            with_info=False,\n",
    "        )\n",
    "        \n",
    "        ds_train = make_ds(ds_train)\n",
    "        ds_test = make_ds(ds_test)\n",
    "        self.iter_train = iter(ds_train)\n",
    "        self.iter_test = iter(ds_test)\n",
    "\n",
    "        # define model\n",
    "        self.model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.RandomFlip(mode='horizontal', input_shape=self.params['input_shape']),\n",
    "            tf.keras.layers.RandomTranslation(0.1, 0.1),\n",
    "            tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=params['input_shape']),\n",
    "            tf.keras.layers.BatchNormalization(fused=True),\n",
    "            tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(fused=True),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(fused=True),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(fused=True),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(fused=True),\n",
    "            tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(fused=True),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(1024, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(self.params['n_classes'], activation='softmax')])\n",
    "    \n",
    "    \n",
    "        # self.model = tf.keras.applications.resnet50.ResNet50(\n",
    "        #     include_top=True, weights='imagenet', input_shape=self.params['input_shape'])\n",
    "        \n",
    "        print(self.model.summary())\n",
    "\n",
    "        # define optimizer\n",
    "        self.optimizer = tfa.optimizers.AdamW(learning_rate=self.params['lr'], weight_decay=params['weight_decay'])\n",
    "        \n",
    "        # maintain history\n",
    "        self.history = []\n",
    "        \n",
    "        # get last layer reps\n",
    "        self.model_last_layer = Model(self.model.input, self.model.layers[-2].output)\n",
    "        \n",
    "        # temperature for platt scaling\n",
    "        self.tau = tf.Variable(tf.ones((1, self.params['n_classes'])))\n",
    "\n",
    "    \n",
    "    def eval_ood(self, X_real, X_fake):\n",
    "        logits = eval_ood_helper(X_real, X_fake, self.tau).numpy()\n",
    "        labels = np.concatenate([np.zeros(128), np.ones(128)])\n",
    "        auc = roc_auc_score(labels, logits)\n",
    "        return auc\n",
    "\n",
    "    \n",
    "    def get_batch_dimension(self, repr): # dimension of the last layer representations from a batch\n",
    "        repr = StandardScaler().fit_transform(repr)\n",
    "        pca = PCA()\n",
    "        pca.fit(repr)\n",
    "        explained_var = pca.explained_variance_\n",
    "        explained_var /= (explained_var.sum() + EPS)\n",
    "        return np.sum([explained_var.cumsum() <= 0.9]) # returns no. of dimensions that account for 90% of variance\n",
    "    \n",
    "    def get_model_weights(self):\n",
    "        params = np.array([])\n",
    "        for layer in t.model.layers:\n",
    "            for wt in layer.trainable_variables: \n",
    "                params = np.concatenate([params, wt.numpy().flatten()], axis=0)\n",
    "        return params\n",
    "    \n",
    "    def set_model_weights(self, param):\n",
    "        idx=0\n",
    "        for layer in self.model.layers:\n",
    "            for wt in layer.trainable_variables: \n",
    "                wt.assign(param[prev:prev+np.prod(wt.shape)].reshape(wt.shape))\n",
    "                idx+=np.prod(wt.shape)\n",
    "                \n",
    "    def evaluate_n_random_batches(self, n=10):\n",
    "        loss = 0.\n",
    "        for _ in range(n):\n",
    "            loss += self.loss_fn(*(self.iter_train), trainable=False)\n",
    "        return loss / n\n",
    "        \n",
    "    def compute_sharpness_metric(self, p=100, delta=0.001):\n",
    "        x_0 = self.get_model_weights() \n",
    "        A = tf.random.normal((x_0.shape[0], p))\n",
    "        proj = tf.linalg.pinv(A) @ x_0\n",
    "        y_min = (tf.math.abs(proj)+1)*delta\n",
    "        y_max = (tf.math.abs(proj)+1)*(-delta)\n",
    "        y_0 = tf.Variable(np.random.zeros(p), trainable=True)\n",
    "        # for LBFS solver, returns func evaluation and gradient\n",
    "        def f(y):\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch()\n",
    "                self.set_model_weights(x_0 + A@y)\n",
    "                loss = - self.evaluate_n_random_batches(n=10)\n",
    "                # we want to maximize the loss hence, the negative sign\n",
    "            return loss, tape.gradient(loss, y)\n",
    "        _, neg_maxf, _ = scipy.optimize.fmin_l_bfgs_b(\n",
    "            f, y_0, bounds=zip(y_min, y_max), maxiter=10)\n",
    "        maxf = -neg_maxf\n",
    "        fx = self.evaluate_n_random_batches(n=10)\n",
    "        sharpness = (maxf - fx) * 100. / (1 + fx)\n",
    "        # reset model weights\n",
    "        self.set_model_weights(x_0)\n",
    "        return sharpness\n",
    "\n",
    "\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        @tf.function\n",
    "        def baseline(X, Y, training):\n",
    "            print(\"Tracing baseline\")\n",
    "            Y_hat = self.model(X, training=training) \n",
    "            ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.argmax(Y_hat, axis=1) == Y, tf.float32)) * 100.\n",
    "            entropy_on_original_point = tf.reduce_mean(-1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat + EPS), axis=1))\n",
    "            return ce_loss, ce_loss, 0., accuracy, entropy_on_original_point\n",
    "\n",
    "        @tf.function\n",
    "        def min_max_cent(X, Y, training, params):\n",
    "            # compute gradient of cross entropy loss wrt X and take a step in +ve direction\n",
    "            # this would try to find a point in the neighborhood of X that maximizes cross entropy\n",
    "            Y_hat = self.model(X, training=training) \n",
    "            ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.argmax(Y_hat, axis=1) == Y, tf.float32)) * 100.\n",
    "            entropy_on_original_point = tf.reduce_mean(-1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat + EPS), axis=1))    \n",
    "            if params['step_size'] > 0.:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    tape.watch(X)\n",
    "                    Y_hat = self.model(X, training=training)\n",
    "                    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "                grads = tape.gradient(loss, X)\n",
    "                grads_norm = tf.norm(tf.reshape(grads, (128, -1)), axis=1)\n",
    "                grads = params['step_size'] * grads / grads_norm[:, None, None, None]\n",
    "                X_perturbed = tf.clip_by_value(X + grads, 0.0, 1.0)\n",
    "            else:\n",
    "                X_perturbed = X\n",
    "            # compute cross entropy at this new point\n",
    "            Y_hat = self.model(X_perturbed, training=training)\n",
    "            loss_adv = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "            return ce_loss + params['lambda'] * loss_adv, ce_loss, loss_adv, accuracy, entropy_on_original_point\n",
    "\n",
    "        @tf.function\n",
    "        def max_min_ent(X, Y, training, params):\n",
    "            # compute grad of entropy wrt X and take a step in negative direction\n",
    "            # this would find a point in the neighborhood of X that would minimize entropy\n",
    "            Y_hat = self.model(X, training=training) \n",
    "            ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.argmax(Y_hat, axis=1) == Y, tf.float32)) * 100.\n",
    "            entropy_on_original_point = tf.reduce_mean(-1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat + EPS), axis=1))    \n",
    "            if params['step_size'] > 0.:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    tape.watch(X)\n",
    "                    Y_hat = self.model(X, training=training)\n",
    "                    exp_neg_entropy = tf.exp(tf.reduce_mean(Y_hat * tf.math.log(Y_hat + EPS), axis=1))\n",
    "                grads = tape.gradient(exp_neg_entropy, X)\n",
    "                grads_norm = tf.norm(tf.reshape(grads, (128, -1)), axis=1)\n",
    "                grads = params['step_size'] * grads / grads_norm[:, None, None, None]      \n",
    "                X_perturbed = tf.clip_by_value(X + grads, 0.0, 1.0)\n",
    "            else:\n",
    "                X_perturbed = X\n",
    "            # compute entropy at this new point and multiply it by -1 (raise to exp. for better grads), since we want to maximize entropy\n",
    "            Y_hat = self.model(X_perturbed, training=training)\n",
    "            entropy = -1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat + EPS), axis=1)\n",
    "            loss_adv = tf.reduce_mean(tf.exp(-1.0 * entropy))\n",
    "            return ce_loss + params['lambda'] * loss_adv, ce_loss, loss_adv, accuracy, entropy_on_original_point\n",
    "\n",
    "        @tf.function\n",
    "        def min_max_KL_unif(X, Y, training, params):\n",
    "            # compute grad of KL(unif, p_\\theta) wrt X and take a step in +ve direction\n",
    "            # this would find a point in the neighborhood of X that would maximize KL\n",
    "            Y_hat = self.model(X, training=training) \n",
    "            ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.argmax(Y_hat, axis=1) == Y, tf.float32)) * 100.\n",
    "            entropy_on_original_point = tf.reduce_mean(-1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat + 1e-3), axis=1)) \n",
    "            if params['step_size'] > 0.:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    tape.watch(X)\n",
    "                    Y_hat = self.model(X, training=training)\n",
    "                    KL_unif = tf.keras.losses.CategoricalCrossentropy(from_logits=False)(\n",
    "                        y_true=tf.ones_like(Y_hat) / params['n_classes'], y_pred=Y_hat)\n",
    "                grads = tape.gradient(KL_unif, X)\n",
    "                grads_norm = tf.norm(tf.reshape(grads, (128, -1)), axis=1)\n",
    "                grads = self.params['step_size'] * grads / grads_norm[:, None, None, None]      \n",
    "                X_perturbed = tf.clip_by_value(X + grads, 0.0, 1.0)\n",
    "            else:\n",
    "                X_perturbed = X\n",
    "            # compute entropy at this new point and multiply it by -1, since we want to maximize entropy\n",
    "            Y_hat = self.model(X_perturbed, training=training)\n",
    "            KL_unif = tf.keras.losses.CategoricalCrossentropy(from_logits=False)(\n",
    "                y_true=tf.ones_like(Y_hat) / params['n_classes'], y_pred=Y_hat)\n",
    "            loss_adv = tf.reduce_mean(KL_unif)\n",
    "            return ce_loss + params['lambda'] * loss_adv, ce_loss, loss_adv, accuracy, entropy_on_original_point\n",
    "\n",
    "        @tf.function\n",
    "        def label_smoothing(X, Y, training, params):\n",
    "            Y_hat = self.model(X, training=training)\n",
    "            ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.argmax(Y_hat, axis=1) == Y, tf.float32)) * 100.\n",
    "            entropy_on_original_point = tf.reduce_mean(-1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat + 1e-3), axis=1)) \n",
    "            Y = tf.one_hot(Y, params['n_classes'])\n",
    "            Y_noisy = Y * (1 - params['label-smoothing-factor']) \n",
    "            Y_noisy += (self.params['label-smoothing-factor'] / tf.cast(params['n_classes'], tf.float32))\n",
    "            noisy_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)(y_true=Y_noisy, y_pred=Y_hat)\n",
    "            return noisy_loss, ce_loss, 0., accuracy, entropy_on_original_point\n",
    "\n",
    "        @tf.function\n",
    "        def get_calibration_metrics(X, Y):\n",
    "            logits = tf.math.log(self.model(X, training=False) + EPS) * tf.repeat(self.tau, X.shape[0], axis=0)\n",
    "            brier = tf.reduce_mean(tfp.stats.brier_score(labels=Y, logits=logits))\n",
    "            ece = tfp.stats.expected_calibration_error(num_bins=20, logits=logits, labels_true=Y)\n",
    "            nll = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(y_true=Y, y_pred=logits)\n",
    "            return brier, ece, nll\n",
    "\n",
    "\n",
    "        @tf.function\n",
    "        def eval_ood_helper(X_real, X_fake):\n",
    "            Y_hat_real = tf.nn.softmax(tf.math.log(self.model(X_real, training=False) + EPS) * tf.repeat(self.tau, X_real.shape[0], axis=0))\n",
    "            entropy_real = -1.0 * tf.reduce_mean(Y_hat_real * tf.math.log(Y_hat_real + EPS), axis=1)\n",
    "            Y_hat_fake = tf.nn.softmax(tf.math.log(self.model(X_fake, training=False) + EPS) * tf.repeat(self.tau, X_fake.shape[0], axis=0))\n",
    "            entropy_fake = -1.0 * tf.reduce_mean(Y_hat_fake * tf.math.log(Y_hat_fake + EPS), axis=1)\n",
    "            return tf.concat([entropy_real, entropy_fake], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "        # define loss function\n",
    "        # computes loss given a model and X, Y\n",
    "        @ tf.function\n",
    "        def loss_fn(X, Y, training, params):\n",
    "\n",
    "            if params['version'] == 'baseline':\n",
    "                return baseline(X, Y, training)\n",
    "            elif params['version'] == 'min-max-cent':\n",
    "                return min_max_cent(X, Y, training, params)\n",
    "            elif params['version'] == 'max-min-ent':\n",
    "                return max_min_ent(X, Y, training, params)\n",
    "            elif params['version'] == 'min-max-KL-unif': \n",
    "                return min_max_KL_unif(X, Y, training, params)\n",
    "            elif params['version'] == 'label-smoothing':\n",
    "                return label_smoothing(X, Y, training, params)\n",
    "            else:\n",
    "                raise ValueError\n",
    "                \n",
    "        # define step function\n",
    "        # computes gradients and applies them\n",
    "        @tf.function\n",
    "        def step_fn(X, Y, params):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss, cent_loss, loss_adv, accuracy, predent = loss_fn(X, Y, True, params)\n",
    "            grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "            return loss, cent_loss, loss_adv, accuracy, predent\n",
    "        \n",
    "        # loop over data n_iters times\n",
    "        for t in tqdm.trange(self.params['n_iters']):\n",
    "            train_loss, train_loss_cent, train_loss_adv, train_acc, train_predent = step_fn(*next(self.iter_train), self.params)    \n",
    "            if t % 1000 == 0:\n",
    "                tf.print(\"Total:\", train_loss, \"CE:\", train_loss_cent, \"Adv:\", train_loss_adv, \"Acc:\", train_acc)\n",
    "            if t % 20 == 0:\n",
    "                test_loss = []\n",
    "                test_loss_cent = []\n",
    "                test_loss_adv = []\n",
    "                test_acc = []\n",
    "                test_predent = []\n",
    "                for _ in range(10):\n",
    "                    res = loss_fn(*next(self.iter_test), False, self.params)\n",
    "                    test_loss.append(res[0].numpy())\n",
    "                    test_loss_cent.append(res[1].numpy())\n",
    "                    test_loss_adv.append(res[2].numpy())\n",
    "                    test_acc.append(res[3].numpy())\n",
    "                    test_predent.append(res[4].numpy())\n",
    "                train_dim = self.get_batch_dimension(self.model_last_layer(next(self.iter_train)[0], training=False))\n",
    "                test_dim = self.get_batch_dimension(self.model_last_layer(next(self.iter_test)[0], training=False))\n",
    "            self.history.append((train_loss.numpy(), np.mean(test_loss), train_acc.numpy(), np.mean(test_acc),\n",
    "                                train_loss_adv.numpy(), np.mean(test_loss_adv), train_predent.numpy(), np.mean(test_predent)))\n",
    "                                # train_dim, test_dim))\n",
    "            \n",
    "            if ('lambda_schedule' in self.params) and ((t+1) % self.params['lambda_schedule']['frequency'] == 0):\n",
    "                self.params['lambda'] *= self.params['lambda_schedule']['factor']\n",
    "                \n",
    "            if ('lr_schedule' in self.params) and ((t+1) % self.params['lr_schedule']['frequency'] == 0):\n",
    "                self.optimizer.lr.assign(self.optimizer.lr * self.params['lr_schedule']['factor'])\n",
    "\n",
    "        self.history = np.array(self.history)\n",
    "\n",
    "        \n",
    "# post hoc platt scaling\n",
    "def calibrate_model(trainer):\n",
    "    # loop over data n_iters times\n",
    "    tau_optimizer = tf.keras.optimizers.Adam(learning_rate=trainer.params['lr-calibrator']*10.)\n",
    "    for t in tqdm.trange(trainer.params['n_iters']//2):\n",
    "        X, Y = next(trainer.iter_train)\n",
    "        Y_pred_logits = tf.math.log(trainer.model(X, training=False) + EPS)\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(trainer.tau)\n",
    "            Y_pred_logits *= tf.repeat(trainer.tau, Y_pred_logits.shape[0], axis=0)\n",
    "            loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(y_true=Y, y_pred=Y_pred_logits)\n",
    "        grad_tau = tape.gradient(loss, trainer.tau)\n",
    "        grad_tau = tf.clip_by_norm(grad_tau, 0.1)\n",
    "        tau_optimizer.apply_gradients([(grad_tau, trainer.tau)])\n",
    "        if t % 1000 == 0:\n",
    "            print(f\"Calibration Loss: {loss}, {trainer.tau}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run training with params {'input_shape': (32, 32, 3), 'n_classes': 10, 'lambda': 0.0, 'lr': 0.0005, 'lr-calibrator': 0.0001, 'step_size': 0.0, 'n_iters': 10000, 'version': 'baseline', 'weight_decay': 0.0001, 'lr_schedule': {'frequency': 8000, 'factor': 0.1}, 'label-smoothing-factor': 0.0}\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "random_flip_13 (RandomFlip)  (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "random_translation_13 (Rando (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_78 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_78 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_79 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_79 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_80 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_80 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_81 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_81 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_82 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_82 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_83 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_83 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 2,397,226\n",
      "Trainable params: 2,396,330\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                              | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing baseline\n",
      "Total: 4.17955542 CE: 4.17955542 Adv: 0 Acc: 8.59375\n",
      "Tracing baseline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████████▌                                                                                                       | 1000/10000 [00:30<03:41, 40.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 0.889262378 CE: 0.889262378 Adv: 0 Acc: 67.1875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████████                                                                                            | 2000/10000 [00:56<02:44, 48.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 0.667754352 CE: 0.667754352 Adv: 0 Acc: 77.34375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██████████████████████████████████▌                                                                                | 3000/10000 [01:22<02:20, 49.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 0.383449674 CE: 0.383449674 Adv: 0 Acc: 85.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████████████████▉                                                                     | 3996/10000 [01:48<02:22, 42.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 0.371704 CE: 0.371704 Adv: 0 Acc: 85.15625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████████▌                                                         | 5000/10000 [02:13<01:47, 46.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 0.414470553 CE: 0.414470553 Adv: 0 Acc: 85.15625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████████████████████████                                              | 6000/10000 [02:39<01:25, 47.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 0.320916235 CE: 0.320916235 Adv: 0 Acc: 89.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|████████████████████████████████████████████████████████████████████████████████▍                                  | 6993/10000 [03:05<01:11, 42.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 0.308200955 CE: 0.308200955 Adv: 0 Acc: 89.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████████████████▉                       | 7993/10000 [03:31<00:47, 41.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 0.295175344 CE: 0.295175344 Adv: 0 Acc: 91.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████▍           | 8997/10000 [03:56<00:22, 44.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 0.180043638 CE: 0.180043638 Adv: 0 Acc: 93.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [04:22<00:00, 38.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "params = {\n",
    "    'input_shape': (32, 32, 3),\n",
    "    'n_classes': 10,\n",
    "    'lambda': 0.0,\n",
    "    'lr': 5e-4,\n",
    "    'lr-calibrator': 1e-4,\n",
    "    'step_size': 0.,\n",
    "    'n_iters': 10000,\n",
    "    'version': 'baseline',\n",
    "    'weight_decay': 0.0001,\n",
    "    'lr_schedule':{\n",
    "        'frequency':8000,\n",
    "        'factor': 0.1\n",
    "    },\n",
    "    'label-smoothing-factor': 0.\n",
    "}\n",
    "baseline_trainer = Trainer(params)\n",
    "baseline_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run training with params {'input_shape': (32, 32, 3), 'n_classes': 10, 'lambda': 5.0, 'lr': 0.0005, 'lr-calibrator': 0.0001, 'step_size': 0.0, 'n_iters': 10000, 'version': 'min-max-KL-unif', 'weight_decay': 0.0001, 'lr_schedule': {'frequency': 8000, 'factor': 0.1}, 'lambda_schedule': {'frequency': 500, 'factor': 1.0}}\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "random_flip_14 (RandomFlip)  (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "random_translation_14 (Rando (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_84 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_84 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_85 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_85 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_86 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_86 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_87 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_87 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_88 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_88 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_89 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_89 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_44 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 2,397,226\n",
      "Trainable params: 2,396,330\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                              | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 22.9821129 CE: 4.05168819 Adv: 3.78608489 Acc: 9.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████████▌                                                                                                        | 999/10000 [00:44<04:22, 34.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 13.6791277 CE: 2.02478671 Adv: 2.33086824 Acc: 47.65625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████████▉                                                                                            | 1997/10000 [01:21<04:37, 28.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 13.5871162 CE: 1.90086699 Adv: 2.33724976 Acc: 57.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██████████████████████████████████▍                                                                                | 2991/10000 [01:54<02:18, 50.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 13.52 CE: 1.7884903 Adv: 2.34630203 Acc: 73.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████████████████▉                                                                     | 3991/10000 [02:15<01:59, 50.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 13.5442724 CE: 1.77408898 Adv: 2.35403681 Acc: 67.1875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|███████████████████████████████████████████████████▏                                                               | 4446/10000 [02:43<02:52, 32.26it/s]"
     ]
    }
   ],
   "source": [
    "# max-min-ent\n",
    "params = {\n",
    "    'input_shape': (32, 32, 3),\n",
    "    'n_classes': 10,\n",
    "    'lambda': 5.,\n",
    "    'lr': 5e-4,\n",
    "    'lr-calibrator': 1e-4,\n",
    "    'step_size': 0.,\n",
    "    'n_iters': 10000,\n",
    "    'version': 'min-max-KL-unif',\n",
    "    'weight_decay': 0.0001,\n",
    "    'lr_schedule':{\n",
    "        'frequency':8000,\n",
    "        'factor': 0.1\n",
    "    },\n",
    "    'lambda_schedule': {\n",
    "        'frequency': 500,\n",
    "        'factor': 1.\n",
    "    }\n",
    "}\n",
    "max_min_ent_trainer = Trainer(params)\n",
    "max_min_ent_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-max-KL-unif\n",
    "params = {\n",
    "    'input_shape': (32, 32, 3),\n",
    "    'n_classes': 10,\n",
    "    'lambda': 0.2,\n",
    "    'lr': 5e-4,\n",
    "    'lr-calibrator': 1e-4,\n",
    "    'step_size': 0.,\n",
    "    'n_iters': 10000,\n",
    "    'version': 'min-max-KL-unif',\n",
    "    'weight_decay': 0.0001,\n",
    "    'lr_schedule':{\n",
    "        'frequency':8000,\n",
    "        'factor': 0.1\n",
    "    },\n",
    "    'lambda_schedule': {\n",
    "        'frequency': 500,\n",
    "        'factor': 1.\n",
    "    }\n",
    "}\n",
    "min_max_KL_unif_trainer = Trainer(params)\n",
    "min_max_KL_unif_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute ID metrics\n",
    "ID_results = [\n",
    "    [np.mean(baseline_trainer.history[:, 2][-20:]), np.mean(baseline_trainer.history[:, 3][-20:])],\n",
    "    [np.mean(max_min_ent_trainer.history[:, 2][-20:]), np.mean(max_min_ent_trainer.history[:, 3][-20:])],    \n",
    "    [np.mean(min_max_KL_unif_trainer.history[:, 2][-20:]), np.mean(min_max_KL_unif_trainer.history[:, 3][-20:])],\n",
    "]\n",
    "plot_ID_metrics(ID_results, tags = ['baseline', 'max_min_ent', 'min_max_KL_unif'])\n",
    "plot_training_metrics(baseline_trainer, min_max_KL_unif_trainer, ['baseline', 'max_min_ent', 'min_max_KL_unif'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting utils\n",
    "def plot_training_metrics(baseline_trainer, our_trainer, tags=['baseline', '']):\n",
    "    \n",
    "    trainer_vec = [baseline_trainer, our_trainer]\n",
    "    \n",
    "    c_vec = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    plt.figure(figsize=(20, 3))\n",
    "\n",
    "    # total loss\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.title(\"Train/Test total loss\")\n",
    "    for idx, (c, trainer) in enumerate(zip(c_vec, trainer_vec)):\n",
    "        name = tags[idx]\n",
    "        plt.plot(gaussian_filter1d(trainer.history[:, 0], 100), '-o', c=c, label='%s - train' % name, markevery=1000, markersize=10)\n",
    "        plt.plot(gaussian_filter1d(trainer.history[:, 1], 100), '-^', c=c, label='%s - val' % name, markevery=1000, markersize=10)\n",
    "    plt.grid()\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('total loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "    # accuracy\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.title(\"Train/Test accuracy\")\n",
    "    for idx, (c, trainer) in enumerate(zip(c_vec, trainer_vec)):\n",
    "        name = tags[idx]\n",
    "        plt.plot(gaussian_filter1d(trainer.history[:, 2], 100), '-o', c=c, label='%s - train' % name, markevery=1000, markersize=10)\n",
    "        plt.plot(gaussian_filter1d(trainer.history[:, 3], 100), '-^', c=c, label='%s - val' % name, markevery=1000, markersize=10)\n",
    "    plt.grid()\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "\n",
    "    # entropy\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.title(\"Train/Test predictive entropy\")\n",
    "    for idx, (c, trainer) in enumerate(zip(c_vec, trainer_vec)):\n",
    "        name = tags[idx]\n",
    "        plt.plot(np.abs(gaussian_filter1d(trainer.history[:, 6], 100)), '-o', c=c, label='%s - train' % name, markevery=1000, markersize=10)\n",
    "        plt.plot(np.abs(gaussian_filter1d(trainer.history[:, 7], 100)), '-^', c=c, label='%s - val' % name, markevery=1000, markersize=10)\n",
    "    plt.grid()\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('Entropy')\n",
    "    \n",
    "    # dimensionality\n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.title(\"Train/Test last-layer-rep dim\")\n",
    "    for idx, (c, trainer) in enumerate(zip(c_vec, trainer_vec)):\n",
    "        name = tags[idx]\n",
    "        plt.plot(gaussian_filter1d(trainer.history[:, 8], 100), '-o', c=c, label='%s - train' % name, markevery=1000, markersize=10)\n",
    "        plt.plot(gaussian_filter1d(trainer.history[:, 9], 100), '-^', c=c, label='%s - val' % name, markevery=1000, markersize=10)\n",
    "    plt.grid()\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('# dims that capture 90% of feature var')\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "def plot_OOD_metrics(results, tags):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for (j, title) in enumerate(['Accuracy OOD $(\\\\uparrow)$', 'OOD AUC $(\\\\uparrow)$', 'Brier $(\\\\downarrow)$', 'ECE  $(\\\\downarrow)$', 'NLL  $(\\\\downarrow)$']):\n",
    "        plt.subplot(2, 3, j + 1)\n",
    "        plt.title(title)\n",
    "        for idx in range(len(tags)):\n",
    "            x = np.arange(len(corruption_type_list))\n",
    "            y = [results[(ctype, idx)][j] for ctype in corruption_type_list]\n",
    "            width = 0.1\n",
    "            offset = width\n",
    "            plt.bar(x + width * (idx + 1) - offset, y, width=width)\n",
    "        plt.xticks(np.arange(len(corruption_type_list)) + 2*width, corruption_type_list, rotation=90)\n",
    "        plt.grid()\n",
    "\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.title('legend')\n",
    "    for idx in range(len(tags)):\n",
    "        plt.scatter(0., 0., label=f'{tags[idx]}')\n",
    "    plt.legend(fontsize=8, loc='lower right')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_ID_metrics(results, tags):\n",
    "    plt.figure(figsize=(5, 6))\n",
    "    for (j, title) in enumerate(['Train Accuracy ID $(\\\\uparrow)$', 'Test Accuracy ID $(\\\\uparrow)$']):\n",
    "        plt.subplot(1, 2, j + 1)\n",
    "        plt.title(title)\n",
    "        width = 1.\n",
    "        offset = width    \n",
    "        for idx in range(len(tags)):\n",
    "            plt.bar(width * (idx + 1), results[idx][j], width=width)\n",
    "        plt.xticks(np.arange(len(tags)) + width, tags, rotation=90)\n",
    "        plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline\n",
    "params = {\n",
    "    'input_shape': (32, 32, 3),\n",
    "    'n_classes': 10,\n",
    "    'lambda': 0.0,\n",
    "    'lr': 3e-4,\n",
    "    'lr-calibrator': 1e-4,\n",
    "    'step_size': 0.,\n",
    "    'n_iters': 12000,\n",
    "    'version': 'NA',\n",
    "    'weight_decay': 0.\n",
    "}\n",
    "baseline_trainer = Trainer(params)\n",
    "baseline_trainer.train()\n",
    "# calibrate_model(baseline_trainer)\n",
    "\n",
    "\n",
    "# min max cross-entropy\n",
    "params = {\n",
    "    'input_shape': (32, 32, 3),\n",
    "    'n_classes': 10,\n",
    "    'lambda': 4.0,\n",
    "    'lr': 3e-4,\n",
    "    'lr-calibrator': 1e-4,\n",
    "    'step_size': 0.1,\n",
    "    'n_iters': 12000,\n",
    "    'version': 'min-max-cent',\n",
    "    'weight_decay': 0.\n",
    "}\n",
    "min_max_cent_trainer = Trainer(params)\n",
    "min_max_cent_trainer.train()\n",
    "# calibrate_model(min_max_cent_trainer)\n",
    "\n",
    "\n",
    "\n",
    "# # max-min-ent\n",
    "params = {\n",
    "    'input_shape': (32, 32, 3),\n",
    "    'n_classes': 10,\n",
    "    'lambda': 4.0,\n",
    "    'lr': 3e-4,\n",
    "    'lr-calibrator': 1e-4,\n",
    "    'step_size': 0.1,\n",
    "    'n_iters': 12000,\n",
    "    'version': 'max-min-ent',\n",
    "    'weight_decay': 0.\n",
    "}\n",
    "max_min_ent_trainer = Trainer(params)\n",
    "max_min_ent_trainer.train()\n",
    "# calibrate_model(max_min_ent_trainer)\n",
    "\n",
    "\n",
    "# min max KL with unif\n",
    "params = {\n",
    "    'input_shape': (32, 32, 3),\n",
    "    'n_classes': 10,\n",
    "    'lambda': 0.1,\n",
    "    'lr': 3e-4,\n",
    "    'lr-calibrator': 1e-4,\n",
    "    'step_size': 0.1,\n",
    "    'n_iters': 12000,\n",
    "    'version': 'min-max-KL-unif',\n",
    "    'weight_decay': 0.\n",
    "}\n",
    "min_max_KL_unif_trainer = Trainer(params)\n",
    "min_max_KL_unif_trainer.train()\n",
    "# calibrate_model(min_max_KL_unif_trainer)\n",
    "\n",
    "# label smoothing\n",
    "params = {\n",
    "    'input_shape': (32, 32, 3),\n",
    "    'n_classes': 10,\n",
    "    'lambda': 0.,\n",
    "    'lr': 3e-4,\n",
    "    'lr-calibrator': 1e-4,\n",
    "    'step_size': 0.,\n",
    "    'n_iters': 12000,\n",
    "    'version': 'label-smoothing',\n",
    "    'label-smoothing-factor': 0.1,\n",
    "    'weight_decay': 0.\n",
    "}\n",
    "label_smoothing_trainer = Trainer(params)\n",
    "label_smoothing_trainer.train()\n",
    "# calibrate_model(label_smoothing_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_trainer.history[:, 2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute ID metrics\n",
    "ID_results = [\n",
    "    [np.mean(baseline_trainer.history[:, 2][-20:]), np.mean(baseline_trainer.history[:, 3][-20:])],\n",
    "    [np.mean(min_max_cent_trainer.history[:, 2][-20:]), np.mean(min_max_cent_trainer.history[:, 3][-20:])],\n",
    "    [np.mean(max_min_ent_trainer.history[:, 2][-20:]), np.mean(max_min_ent_trainer.history[:, 3][-20:])],\n",
    "    [np.mean(min_max_KL_unif_trainer.history[:, 2][-20:]), np.mean(min_max_KL_unif_trainer.history[:, 3][-20:])],\n",
    "    [np.mean(label_smoothing_trainer.history[:, 2][-20:]), np.mean(label_smoothing_trainer.history[:, 3][-20:])],\n",
    "    #     [np.mean(min_max_cent_trainer_alp0.history[:, 2][-20:]), np.mean(min_max_cent_trainer_alp0.history[:, 3][-50:])],\n",
    "    #     [np.mean(max_min_ent_trainer_alp0.history[:, 2][-20:]), np.mean(max_min_ent_trainer_alp0.history[:, 3][-50:])],\n",
    "    #     [np.mean(min_max_KL_unif_trainer_alp0.history[:, 2][-20:]), np.mean(min_max_KL_unif_trainer_alp0.history[:, 3][-50:])],\n",
    "]\n",
    "plot_ID_metrics(ID_results, tags = ['baseline', 'min-max-cent', 'max-min-ent', 'min-max-KL-unif', 'label-smoothing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plot_training_metrics(baseline_trainer, min_max_cent_trainer, ['baseline', 'min-max-cent'])\n",
    "plot_training_metrics(baseline_trainer, max_min_ent_trainer, ['baseline', 'max-min-ent'])\n",
    "plot_training_metrics(baseline_trainer, min_max_KL_unif_trainer, ['baseline', 'min-max-KL-unif'])\n",
    "plot_training_metrics(baseline_trainer, label_smoothing_trainer, ['baseline', 'label-smoothing'])\n",
    "\n",
    "# plot_training_metrics(baseline_trainer, min_max_cent_trainer_alp0, ['baseline', 'min-max-cent-alp0'])\n",
    "# plot_training_metrics(baseline_trainer, max_min_ent_trainer_alp0, ['baseline', 'max-min-ent-alp0'])\n",
    "# plot_training_metrics(baseline_trainer, min_max_KL_unif_trainer_alp0, ['baseline', 'min-max-KL-unif-alp0'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute OOD metrics\n",
    "trainer_vec = [baseline_trainer, min_max_KL_unif_trainer]\n",
    "#                min_max_cent_trainer, max_min_ent_trainer, min_max_KL_unif_trainer, label_smoothing_trainer]\n",
    "tags = ['baseline', 'min-max-KL-unif']\n",
    "#         'min-max-cent', 'max-min-ent', 'min-max-KL-unif', 'label-smoothing']\n",
    "\n",
    "results = {}\n",
    "corruption_type_list = ['brightness_1', 'elastic_5', 'fog_5', 'frost_5', 'frosted_glass_blur_5']\n",
    "\n",
    "for idx, trainer in enumerate(trainer_vec):\n",
    "    print(f\"Trainer {idx + 1}\")\n",
    "    for corruption_type in corruption_type_list:\n",
    "        print(corruption_type)\n",
    "        \n",
    "        # load corrupted dataset\n",
    "        (ds_corrupted,) = tfds.load(\n",
    "            'cifar10_corrupted/%s' % corruption_type,\n",
    "            split=['test'],\n",
    "            shuffle_files=True,\n",
    "            as_supervised=True,\n",
    "            with_info=False,\n",
    "        )\n",
    "        \n",
    "        # make dataset and get iterator\n",
    "        ds_corrupted = make_ds(ds_corrupted)\n",
    "        iter_corrupted = iter(ds_corrupted)\n",
    "\n",
    "        # record metrics\n",
    "        acc_OOD_vec = []\n",
    "        auc_vec = []\n",
    "        brier_vec = []\n",
    "        ece_vec = []\n",
    "        nll_vec = []\n",
    "        \n",
    "        # average over a 100 batches\n",
    "        for _ in tqdm.trange(100):\n",
    "            X_test, Y_test = next(trainer.iter_test)\n",
    "            X_corrupted, Y_corrupted = next(iter_corrupted)\n",
    "            _, _, test_acc_OOD, _ = loss_fn(X_corrupted, Y_corrupted, False)\n",
    "            test_auc = trainer.eval_ood(X_test, X_corrupted)\n",
    "            brier, ece, nll = get_calibration_metrics(X_corrupted, Y_corrupted, self.tau)\n",
    "\n",
    "            acc_OOD_vec.append(test_acc_OOD.numpy())\n",
    "            auc_vec.append(test_auc)\n",
    "            brier_vec.append(brier.numpy())\n",
    "            ece_vec.append(ece.numpy())\n",
    "            nll_vec.append(nll.numpy())\n",
    "        \n",
    "        results[(corruption_type, idx)] = (np.mean(acc_OOD_vec), np.mean(auc_vec), np.mean(brier_vec), np.mean(ece_vec), np.mean(nll_vec))\n",
    "\n",
    "plot_OOD_metrics(results, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_OOD_metrics(results, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rt = Trainer(params)\n",
    "rt.compute_sharpness_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    loss, _, _, _ = t.loss_fn(*next(t.iter_train), training=False)\n",
    "grad = np.concatenate([g.numpy().flatten() for g in tape.gradient(loss, t.model.trainable_variables)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_sum(tf.linalg.pinv(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tf.random.normal((2396330, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = tf.Variable(np.random.randn((2396330, 100)), trainable=False)\n",
    "tf.reduce_sum(tf.linalg.pinv(A) @ tf.random.normal((2396330,))[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 3\n",
    "@tf.function\n",
    "def a_function_with_python_side_effect_2(x, **xargs):\n",
    "  print(\"Tracing!\") # An eager-only side effect.\n",
    "  return a + x + xargs['0'] * xargs['1']['b'] + (tf.constant(2.) if ('version' in xargs) and (xargs['version'] == 'add2') else tf.constant(0.))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def a_function_with_python_side_effect(x, **xargs):\n",
    "#   print(\"Tracing!\") # An eager-only side effect.\n",
    "  return a_function_with_python_side_effect_2(x, **xargs)\n",
    "\n",
    "# This is traced the first time.\n",
    "\n",
    "\n",
    "def conv_to_tensors(p):\n",
    "    return {k: tf.constant(v) if type(v) != dict else conv_to_tensors(v) for k, v in p.items()}\n",
    "\n",
    "x = tf.Variable(20.)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    p = {'0':1., '1': {'a': 2., 'b':3.}, 'version': 'dontadd2'}\n",
    "    print(a_function_with_python_side_effect(x, **conv_to_tensors(p)))\n",
    "    # The second time through, you won't see the side effect.\n",
    "#     x.assign(10)\n",
    "#     p = {'0':3, '1':{'a': 2, 'b':-3}, 'version': 'add2'}\n",
    "#     print(a_function_with_python_side_effect(x, **conv_to_tensors(p)))\n",
    "# grad = tape.gradient(v, x)\n",
    "# grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = tf.Variable(2.)\n",
    "# with tf.GradientTape() as tape:\n",
    "#     tape.watch(x)\n",
    "# for i in range(10):\n",
    "#     xargs['0'] = 2. * i\n",
    "#     x.assign(i*1.0)\n",
    "    \n",
    "xargs = {'0':1., '1': {'a': 2., 'b':3.}, 'version': 'dontadd2'}\n",
    "\n",
    "@tf.function\n",
    "def a_function_with_python_side_effect(x, xargs):\n",
    "  print(\"Tracing!\") # An eager-only side effect.\n",
    "  return x + xargs['0'] * xargs['1']['b'] + (tf.constant(2.) if ('version' in xargs) and (xargs['version'] == 'add2') else tf.constant(0.))\n",
    "\n",
    "\n",
    "print(a_function_with_python_side_effect(x, xargs))\n",
    "\n",
    "# xargs['0']=2\n",
    "# tf.function(a_function_with_python_side_effect).get_concrete_function(x)\n",
    "print(a_function_with_python_side_effect(x, xargs))\n",
    "\n",
    "\n",
    "# tf.function(evaluate).get_concrete_function(new_model, x)\n",
    "# print(a_function_with_python_side_effect(x, **conv_to_tensors(p)))\n",
    "    \n",
    "    # The second time through, you won't see the side effect.\n",
    "#     x.assign(10)\n",
    "#     p = {'0':3, '1':{'a': 2, 'b':-3}, 'version': 'add2'}\n",
    "#     print(a_function_with_python_side_effect(x, **conv_to_tensors(p)))\n",
    "# grad = tape.gradient(v, x)\n",
    "# grad"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "name": "predictive entropy.ipynb",
   "provenance": [
    {
     "file_id": "1JQbpV3_b5nJeSwR-vLJ-t9xoCu33COGH",
     "timestamp": 1630008033655
    },
    {
     "file_id": "1l0iWRfgfMvCsvz6Bqu-RcHKkeHEF48qb",
     "timestamp": 1628698030421
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
