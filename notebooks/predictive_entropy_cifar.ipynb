{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PQcCQ7kyNX5E"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import tqdm\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow_probability as tfp\n",
    "from scipy.ndimage import gaussian_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfsyNoi-HSo1"
   },
   "source": [
    "### cifar experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to create TF datasets\n",
    "\n",
    "def normalize_img(image, label):\n",
    "    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "    image = tf.cast(image, tf.float32) / 255.\n",
    "    # image = tf.reshape(image, (-1,))\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def make_ds(ds):\n",
    "    ds = ds.map(\n",
    "        normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.cache()\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.shuffle(int(1e5))\n",
    "    ds = ds.batch(128)\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset, iterate over it and train, return model and metrics\n",
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        print(f\"\\nRun training with params {self.params}\")\n",
    "        \n",
    "        # create CIFAR dataset and get iterators\n",
    "        ds_train, ds_test = out = tfds.load(\n",
    "            'cifar10',\n",
    "            split=['train', 'test'],\n",
    "            shuffle_files=True,\n",
    "            as_supervised=True,\n",
    "            with_info=False,\n",
    "        )\n",
    "        ds_train = make_ds(ds_train)\n",
    "        ds_test = make_ds(ds_test)\n",
    "        self.iter_train = iter(ds_train)\n",
    "        self.iter_test = iter(ds_test)\n",
    "\n",
    "        # define model\n",
    "        self.model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=params['input_shape']),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.params['n_classes'], activation='softmax')])\n",
    "        print(self.model.summary())\n",
    "\n",
    "        # define optimizer\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.params['lr'])\n",
    "        \n",
    "        # maintain history\n",
    "        self.history = []\n",
    "        \n",
    "        # get last layer reps\n",
    "        self.model_last_layer = Model(self.model.input, self.model.layers[-2].output)\n",
    "        \n",
    "\n",
    "\n",
    "    # define loss function\n",
    "    # computes loss given a model and X, Y\n",
    "    @tf.function\n",
    "    def loss_fn(self, X, Y):\n",
    "        Y_hat = self.model(X) \n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.argmax(Y_hat, axis=1) == Y, tf.float32)) * 100.\n",
    "        entropy_on_original_point = tf.reduce_mean(-1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat), axis=1))\n",
    "\n",
    "        if self.params['lambda'] == 0:\n",
    "            return loss, 0., accuracy, entropy_on_original_point\n",
    "\n",
    "        # Do PGD\n",
    "        step_size = params['step_size']\n",
    "        \n",
    "        if self.params['version'] == 'min-max-cent':\n",
    "            # compute gradient of cross entropy loss wrt X and take a step in +ve direction\n",
    "            # this would try to find a point in the neighborhood of X that maximizes cross entropy\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(X)\n",
    "                Y_hat = self.model(X)\n",
    "                loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "            grads = tape.gradient(loss, X)\n",
    "            grads_norm = tf.norm(tf.reshape(grads, (128, -1)), axis=1)\n",
    "            grads = step_size * grads / grads_norm[:, None, None, None]\n",
    "            entropy_on_original_point = tf.reduce_mean(-1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat), axis=1))\n",
    "            X_perturbed = tf.clip_by_value(X + grads, 0.0, 1.0)\n",
    "            # compute cross entropy at this new point\n",
    "            Y_hat = self.model(X_perturbed)  \n",
    "            loss_adv = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "            return loss + self.params['lambda'] * loss_adv, loss_adv, accuracy, entropy_on_original_point\n",
    "\n",
    "        elif self.params['version'] == 'max-min-ent':\n",
    "            # compute grad of entropy wrt X and take a step in negative direction\n",
    "            # this would find a point in the neighborhood of X that would minimize entropy\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(X)\n",
    "                Y_hat = self.model(X)\n",
    "                exp_neg_entropy = tf.exp(tf.reduce_mean(Y_hat * tf.math.log(Y_hat), axis=1))\n",
    "                entropy_on_original_point = tf.reduce_mean(-1. * tf.reduce_mean(Y_hat * tf.math.log(Y_hat), axis=1))\n",
    "            grads = tape.gradient(exp_neg_entropy, X)\n",
    "            grads_norm = tf.norm(tf.reshape(grads, (128, -1)), axis=1)\n",
    "            grads = step_size * grads / grads_norm[:, None, None, None]      \n",
    "            X_perturbed = tf.clip_by_value(X + grads, 0.0, 1.0)\n",
    "            # compute entropy at this new point and multiply it by -1 (raise to exp. for better grads), since we want to maximize entropy\n",
    "            Y_hat = self.model(X_perturbed)\n",
    "            entropy = -1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat), axis=1)\n",
    "            loss_adv = tf.reduce_mean(tf.exp(-1.0 * entropy))\n",
    "            return loss + self.params['lambda'] * loss_adv, loss_adv, accuracy, entropy_on_original_point\n",
    "        \n",
    "        elif self.params['version'] == 'min-max-KL-unif':\n",
    "            # compute grad of KL(unif, p_\\theta) wrt X and take a step in +ve direction\n",
    "            # this would find a point in the neighborhood of X that would maximize KL\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(X)\n",
    "                Y_hat = self.model(X)\n",
    "                KL_unif = tf.keras.losses.CategoricalCrossentropy(from_logits=False)(y_true=tf.ones_like(Y_hat) / params['n_classes'], y_pred=Y_hat)\n",
    "            grads = tape.gradient(KL_unif, X)\n",
    "            grads_norm = tf.norm(tf.reshape(grads, (128, -1)), axis=1)\n",
    "            grads = step_size * grads / grads_norm[:, None, None, None]      \n",
    "            entropy_on_original_point = tf.reduce_mean(-1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat), axis=1))\n",
    "            X_perturbed = tf.clip_by_value(X + grads, 0.0, 1.0)\n",
    "            # compute entropy at this new point and multiply it by -1, since we want to maximize entropy\n",
    "            Y_hat = self.model(X_perturbed)\n",
    "            KL_unif = tf.keras.losses.CategoricalCrossentropy(from_logits=False)(y_true=tf.ones_like(Y_hat) / params['n_classes'], y_pred=Y_hat)\n",
    "            loss_adv = tf.reduce_mean(KL_unif)\n",
    "            return loss + self.params['lambda'] * loss_adv, loss_adv, accuracy, entropy_on_original_point\n",
    "\n",
    "        elif self.params['version'] == 'min-max-cent-label-smooth':\n",
    "            # compute grad of Cross Entropy(noisy_label, p_\\theta) wrt X and take a step in +ve direction\n",
    "            # this would find a point in the neighborhood of X that would maximize Cross Entropy\n",
    "            Y = tf.one_hot(Y, self.params['n_classes'])\n",
    "            Y_noisy = Y * (1 - self.params['label-smoothing-factor']) \n",
    "            Y_noisy += (self.params['label-smoothing-factor'] / Y.shape[1])\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(X)\n",
    "                Y_hat = self.model(X)\n",
    "                cent_noisy = tf.keras.losses.CategoricalCrossentropy(from_logits=False)(y_true=Y_noisy, y_pred=Y_hat)    \n",
    "            grads = tape.gradient(cent_noisy, X)\n",
    "            grads_norm = tf.norm(tf.reshape(grads, (128, -1)), axis=1)\n",
    "            grads = step_size * grads / grads_norm[:, None, None, None]      \n",
    "            entropy_on_original_point = tf.reduce_mean(-1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat), axis=1))\n",
    "            X_perturbed = tf.clip_by_value(X + grads, 0.0, 1.0)\n",
    "            # compute entropy at this new point and multiply it by -1, since we want to maximize entropy\n",
    "            Y_hat = self.model(X_perturbed)\n",
    "            cent_noisy = tf.keras.losses.CategoricalCrossentropy(from_logits=False)(y_true=Y_noisy, y_pred=Y_hat)\n",
    "            loss_adv = tf.reduce_mean(cent_noisy)\n",
    "            return loss + self.params['lambda'] * loss_adv, loss_adv, accuracy, entropy_on_original_point\n",
    "        \n",
    "        \n",
    "        elif self.params['version'] == 'label-smoothing':\n",
    "            Y_hat = self.model(X)\n",
    "            Y = tf.one_hot(Y, self.params['n_classes'])\n",
    "            Y_noisy = Y * (1 - self.params['label-smoothing-factor']) \n",
    "            Y_noisy += (self.params['label-smoothing-factor'] / Y.shape[1])\n",
    "            loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)(y_true=Y_noisy, y_pred=Y_hat)\n",
    "            entropy_on_original_point = tf.reduce_mean(-1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat), axis=1))\n",
    "            return loss, 0., accuracy, entropy_on_original_point\n",
    "            \n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "\n",
    "    # define step function\n",
    "    # computes gradients and applies them\n",
    "    @tf.function\n",
    "    def step_fn(self, X_train, Y_train):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss, loss_adv, accuracy, predent = self.loss_fn(X_train, Y_train)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss, loss_adv, accuracy, predent\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def eval_ood_helper(self, X_real, X_fake):\n",
    "        Y_hat_real = self.model(X_real)\n",
    "        entropy_real = -1.0 * tf.reduce_mean(Y_hat_real * tf.math.log(Y_hat_real), axis=1)\n",
    "        Y_hat_fake = self.model(X_fake)\n",
    "        entropy_fake = -1.0 * tf.reduce_mean(Y_hat_fake * tf.math.log(Y_hat_fake), axis=1)\n",
    "        return tf.concat([entropy_real, entropy_fake], axis=0)\n",
    "\n",
    "    \n",
    "    def eval_ood(self, X_real, X_fake):\n",
    "        logits = trainer.eval_ood_helper(X_real, X_fake).numpy()\n",
    "        labels = np.concatenate([np.zeros(128), np.ones(128)])\n",
    "        auc = roc_auc_score(labels, logits)\n",
    "        return auc\n",
    "\n",
    "    @tf.function\n",
    "    def get_calibration_metrics(self, X, Y):\n",
    "        logits = tf.math.log(self.model(X))\n",
    "        brier = tf.reduce_mean(tfp.stats.brier_score(labels=Y, logits=logits))\n",
    "        ece = tfp.stats.expected_calibration_error(num_bins=20, logits=logits, labels_true=Y)\n",
    "        nll = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(y_true=Y, y_pred=logits)\n",
    "        return brier, ece, nll\n",
    "    \n",
    "    \n",
    "    def get_batch_dimension(self, X): # dimension of the last layer representations from a batch\n",
    "        repr = self.model_last_layer(X).numpy()\n",
    "        repr = StandardScaler().fit_transform(repr)\n",
    "        pca = PCA()\n",
    "        pca.fit(repr)\n",
    "        explained_var = pca.explained_variance_\n",
    "        explained_var /= explained_var.sum()\n",
    "        return np.sum([explained_var.cumsum() <= 0.9]) # returns no. of dimensions that account for 90% of variance\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        # loop over data n_iters times\n",
    "        for t in tqdm.trange(self.params['n_iters']):\n",
    "            train_loss, train_loss_adv, train_acc, train_predent = self.step_fn(*next(self.iter_train))\n",
    "            if t % 1000 == 0:\n",
    "                print(train_loss, train_loss_adv)\n",
    "\n",
    "            if t % 10 == 0:\n",
    "                test_loss, test_loss_adv, test_acc, test_predent = self.loss_fn(*next(self.iter_test))\n",
    "                train_dim = self.get_batch_dimension(next(self.iter_train)[0])\n",
    "                test_dim = self.get_batch_dimension(next(self.iter_test)[0])\n",
    "            self.history.append((train_loss.numpy(), test_loss.numpy(), train_acc.numpy(), test_acc.numpy(),\n",
    "                                train_loss_adv.numpy(), test_loss_adv.numpy(), train_predent.numpy(), test_predent.numpy(), \n",
    "                                train_dim, test_dim))\n",
    "\n",
    "        self.history = np.array(self.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting utils\n",
    "\n",
    "def plot_training_metrics(baseline_trainer, our_trainer, tags=['baseline', '']):\n",
    "    \n",
    "    trainer_vec = [baseline_trainer, our_trainer]\n",
    "    \n",
    "    c_vec = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    plt.figure(figsize=(20, 3))\n",
    "    \n",
    "    # accuracy\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.title(\"Train/Test accuracy\")\n",
    "    for idx, (c, trainer) in enumerate(zip(c_vec, trainer_vec)):\n",
    "        name = tags[idx]\n",
    "        plt.plot(gaussian_filter1d(trainer.history[:, 2], 100), '-o', c=c, label='%s - train' % name, markevery=1000, markersize=10)\n",
    "        plt.plot(gaussian_filter1d(trainer.history[:, 3], 100), '-^', c=c, label='%s - val' % name, markevery=1000, markersize=10)\n",
    "    plt.grid()\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "\n",
    "    # entropy\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.title(\"Train/Test predictive entropy\")\n",
    "    for idx, (c, trainer) in enumerate(zip(c_vec, trainer_vec)):\n",
    "        name = tags[idx]\n",
    "        plt.plot(np.abs(gaussian_filter1d(trainer.history[:, 6], 100)), '-o', c=c, label='%s - train' % name, markevery=1000, markersize=10)\n",
    "        plt.plot(np.abs(gaussian_filter1d(trainer.history[:, 7], 100)), '-^', c=c, label='%s - val' % name, markevery=1000, markersize=10)\n",
    "    plt.grid()\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('Entropy')\n",
    "    \n",
    "    # dimensionality\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.title(\"Train/Test last-layer-rep dim\")\n",
    "    for idx, (c, trainer) in enumerate(zip(c_vec, trainer_vec)):\n",
    "        name = tags[idx]\n",
    "        plt.plot(gaussian_filter1d(trainer.history[:, 8], 100), '-o', c=c, label='%s - train' % name, markevery=1000, markersize=10)\n",
    "        plt.plot(gaussian_filter1d(trainer.history[:, 9], 100), '-^', c=c, label='%s - val' % name, markevery=1000, markersize=10)\n",
    "    plt.grid()\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('Adv Loss')\n",
    "\n",
    "\n",
    "#     # loss\n",
    "#     plt.subplot(1, 4, 3)\n",
    "#     plt.title(\"Train/Test cross entropy loss\")\n",
    "#     for idx, (c, trainer) in enumerate(zip(c_vec, trainer_vec)):\n",
    "#         name = tags[idx]\n",
    "#         plt.plot(gaussian_filter1d(trainer.history[:, 0], 100), '-o', c=c, label='%s - train' % name, markevery=1000, markersize=10)\n",
    "#         plt.plot(gaussian_filter1d(trainer.history[:, 1], 100), '-^', c=c, label='%s - val' % name, markevery=1000, markersize=10)\n",
    "#     plt.grid()\n",
    "#     plt.xlabel('iterations')\n",
    "#     plt.ylabel('Loss')\n",
    "    \n",
    "        \n",
    "#     # adv loss\n",
    "#     plt.subplot(1, 4, 4)\n",
    "#     plt.title(\"Train/Test adversarial loss\")\n",
    "#     for idx, (c, trainer) in enumerate(zip(c_vec, trainer_vec)):\n",
    "#         name = tags[idx]\n",
    "#         plt.plot(np.abs(gaussian_filter1d(trainer.history[:, 4], 100)), '-o', c=c, label='%s - train' % name, markevery=1000, markersize=10)\n",
    "#         plt.plot(np.abs(gaussian_filter1d(trainer.history[:, 5], 100)), '-^', c=c, label='%s - val' % name, markevery=1000, markersize=10)\n",
    "#     plt.grid()\n",
    "#     plt.xlabel('iterations')\n",
    "#     plt.ylabel('Adv Loss')\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "def plot_OOD_metrics(results, tags):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for (j, title) in enumerate(['Accuracy OOD $(\\\\uparrow)$', 'OOD AUC $(\\\\uparrow)$', 'Brier $(\\\\downarrow)$', 'ECE  $(\\\\downarrow)$', 'NLL  $(\\\\downarrow)$']):\n",
    "        plt.subplot(2, 3, j + 1)\n",
    "        plt.title(title)\n",
    "        for idx in range(len(tags)):\n",
    "            x = np.arange(len(corruption_type_list))\n",
    "            y = [results[(ctype, idx)][j] for ctype in corruption_type_list]\n",
    "            width = 0.25\n",
    "            offset = width\n",
    "            plt.bar(x + width * (idx + 1) - offset, y, width=width, label=f'{tags[idx]}')\n",
    "        plt.xticks(np.arange(len(corruption_type_list)), corruption_type_list, rotation=90)\n",
    "        plt.grid()\n",
    "        if j == 0:\n",
    "            plt.legend(fontsize=12, loc='lower right')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run training with params {'input_shape': (32, 32, 3), 'n_classes': 10, 'lambda': 0.0, 'lr': 0.0005, 'step_size': 0.0, 'n_iters': 12000, 'version': 'NA'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-05 09:51:28.764960: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-10-05 09:51:28.765353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-05 09:51:28.766101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2021-10-05 09:51:28.766152: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-10-05 09:51:28.766232: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-10-05 09:51:28.766254: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-10-05 09:51:28.766274: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-10-05 09:51:28.766293: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-10-05 09:51:28.766312: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-10-05 09:51:28.766331: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-10-05 09:51:28.766350: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-10-05 09:51:28.766463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-05 09:51:28.767158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-05 09:51:28.767741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-10-05 09:51:28.767786: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-10-05 09:51:29.386326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-10-05 09:51:29.386354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2021-10-05 09:51:29.386362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2021-10-05 09:51:29.386915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-05 09:51:29.406296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-05 09:51:29.406985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-05 09:51:29.407538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14957 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 122,570\n",
      "Trainable params: 122,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/12000 [00:00<?, ?it/s]2021-10-05 09:51:29.766382: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-10-05 09:51:29.766977: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2199995000 Hz\n",
      "2021-10-05 09:51:33.369259: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-10-05 09:51:33.670315: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-10-05 09:51:33.673473: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.3119864, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███▏                                 | 1024/12000 [00:10<00:52, 209.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.1929727, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████▎                              | 2030/12000 [00:15<00:44, 223.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0098134, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████▎                           | 3030/12000 [00:19<00:42, 212.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.72637284, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|████████████▍                        | 4038/12000 [00:24<00:36, 218.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.80525386, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|███████████████▍                     | 5024/12000 [00:29<00:32, 216.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.90484774, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████▌                  | 6037/12000 [00:33<00:27, 217.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.77912486, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████████████████████▋               | 7021/12000 [00:38<00:23, 214.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.75604033, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████▋            | 8023/12000 [00:43<00:18, 215.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.5690528, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████▉         | 9041/12000 [00:47<00:13, 213.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.7019794, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|██████████████████████████████      | 10022/12000 [00:52<00:09, 207.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.61864173, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████████████████████████████   | 11031/12000 [00:57<00:04, 211.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.49456102, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 12000/12000 [01:01<00:00, 194.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run training with params {'input_shape': (32, 32, 3), 'n_classes': 10, 'lambda': 10.0, 'lr': 0.0005, 'step_size': 0.2, 'n_iters': 12000, 'version': 'min-max-cent'}\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 122,570\n",
      "Trainable params: 122,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/12000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(25.611135, shape=(), dtype=float32) tf.Tensor(2.3302844, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███▏                                  | 1019/12000 [00:14<01:53, 97.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(15.611682, shape=(), dtype=float32) tf.Tensor(1.4344823, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███▉                                  | 1226/12000 [00:16<01:48, 99.46it/s]"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "params = {\n",
    "    'input_shape': (32, 32, 3),\n",
    "    'n_classes': 10,\n",
    "    'lambda': 0.0,\n",
    "    'lr': 5e-4,\n",
    "    'step_size': 0.,\n",
    "    'n_iters': 12000,\n",
    "    'version': 'NA'\n",
    "}\n",
    "baseline_trainer = Trainer(params)\n",
    "baseline_trainer.train()\n",
    "\n",
    "\n",
    "# min max cross-entropy\n",
    "params = {\n",
    "    'input_shape': (32, 32, 3),\n",
    "    'n_classes': 10,\n",
    "    'lambda': 10.0,\n",
    "    'lr': 5e-4,\n",
    "    'step_size': 0.2,\n",
    "    'n_iters': 12000,\n",
    "    'version': 'min-max-cent'\n",
    "}\n",
    "min_max_cent_trainer = Trainer(params)\n",
    "min_max_cent_trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "# max-min-ent\n",
    "params = {\n",
    "    'input_shape': (32, 32, 3),\n",
    "    'n_classes': 10,\n",
    "    'lambda': 10.0,\n",
    "    'lr': 5e-4,\n",
    "    'step_size': 0.2,\n",
    "    'n_iters': 12000,\n",
    "    'version': 'max-min-ent'\n",
    "}\n",
    "max_min_ent_trainer = Trainer(params)\n",
    "max_min_ent_trainer.train()\n",
    "\n",
    "\n",
    "# min max KL with unif\n",
    "params = {\n",
    "    'input_shape': (32, 32, 3),\n",
    "    'n_classes': 10,\n",
    "    'lambda': 0.25,\n",
    "    'lr': 5e-4,\n",
    "    'step_size': 0.1,\n",
    "    'n_iters': 12000,\n",
    "    'version': 'min-max-KL-unif'\n",
    "}\n",
    "min_max_KL_unif_trainer = Trainer(params)\n",
    "min_max_KL_unif_trainer.train()\n",
    "\n",
    "\n",
    "# label smoothing\n",
    "params = {\n",
    "    'input_shape': (32, 32, 3),\n",
    "    'n_classes': 10,\n",
    "    'lambda': 0.,\n",
    "    'lr': 5e-4,\n",
    "    'step_size': 0.,\n",
    "    'n_iters': 12000,\n",
    "    'version': 'label-smoothing',\n",
    "    'label-smoothing-factor': 0.1\n",
    "}\n",
    "label_smoothing_trainer = Trainer(params)\n",
    "label_smoothing_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plot_training_metrics(baseline_trainer, min_max_cent_trainer, ['baseline', 'min-max-cent'])\n",
    "plot_training_metrics(baseline_trainer, max_min_ent_trainer, ['baseline', 'max-min-ent'])\n",
    "plot_training_metrics(baseline_trainer, min_max_KL_unif_trainer, ['baseline', 'min-max-KL-unif'])\n",
    "plot_training_metrics(baseline_trainer, label_smoothing_trainer, ['baseline', 'label-smoothing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute OOD metrics\n",
    "trainer_vec = [baseline_trainer, our_trainer_1, our_trainer_2]\n",
    "results = {}\n",
    "corruption_type_list = ['brightness_1', 'elastic_5', 'fog_5', 'frost_5', 'frosted_glass_blur_5']\n",
    "\n",
    "for idx, trainer in enumerate(trainer_vec):\n",
    "    print(f\"Trainer {idx + 1}\")\n",
    "    for corruption_type in corruption_type_list:\n",
    "        print(corruption_type)\n",
    "        \n",
    "        # load corrupted dataset\n",
    "        (ds_corrupted,) = tfds.load(\n",
    "            'cifar10_corrupted/%s' % corruption_type,\n",
    "            split=['test'],\n",
    "            shuffle_files=True,\n",
    "            as_supervised=True,\n",
    "            with_info=False,\n",
    "        )\n",
    "        \n",
    "        # make dataset and get iterator\n",
    "        ds_corrupted = make_ds(ds_corrupted)\n",
    "        iter_corrupted = iter(ds_corrupted)\n",
    "\n",
    "        # record metrics\n",
    "        acc_OOD_vec = []\n",
    "        auc_vec = []\n",
    "        brier_vec = []\n",
    "        ece_vec = []\n",
    "        nll_vec = []\n",
    "        \n",
    "        # average over a 100 batches\n",
    "        for _ in tqdm.trange(100):\n",
    "            X_test, Y_test = next(trainer.iter_test)\n",
    "            X_corrupted, Y_corrupted = next(iter_corrupted)\n",
    "            _, _, test_acc_OOD = trainer.loss_fn(X_corrupted, Y_corrupted)\n",
    "            test_auc = trainer.eval_ood(X_test, X_corrupted)\n",
    "            brier, ece, nll = trainer.get_calibration_metrics(X_corrupted, Y_corrupted)\n",
    "\n",
    "            acc_OOD_vec.append(test_acc_OOD.numpy())\n",
    "            auc_vec.append(test_auc)\n",
    "            brier_vec.append(brier.numpy())\n",
    "            ece_vec.append(ece.numpy())\n",
    "            nll_vec.append(nll.numpy())\n",
    "        \n",
    "        results[(corruption_type, idx)] = (np.mean(acc_OOD_vec), np.mean(auc_vec), np.mean(brier_vec), np.mean(ece_vec), np.mean(nll_vec))\n",
    "\n",
    "plot_OOD_metrics(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "name": "predictive entropy.ipynb",
   "provenance": [
    {
     "file_id": "1JQbpV3_b5nJeSwR-vLJ-t9xoCu33COGH",
     "timestamp": 1630008033655
    },
    {
     "file_id": "1l0iWRfgfMvCsvz6Bqu-RcHKkeHEF48qb",
     "timestamp": 1628698030421
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
