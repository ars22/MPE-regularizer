{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQcCQ7kyNX5E"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "from tiny_imagenet import TinyImagenetDataset\n",
    "import tqdm\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow_probability as tfp\n",
    "from scipy.ndimage import gaussian_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "\n",
    "def tf_random_rotate_image(image, label):\n",
    "  im_shape = image.shape\n",
    "  [image,] = tf.py_function(random_rotate_image, [image], [tf.float32])\n",
    "  image.set_shape(im_shape)\n",
    "  return image, label\n",
    "\n",
    "\n",
    "def augment(image, label):\n",
    "    # print(ia.is_np_array(image.numpy()))\n",
    "    # tf.print(image)\n",
    "    sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "    seq = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5),\n",
    "    iaa.Flipud(0.2),\n",
    "    sometimes(iaa.GaussianBlur(sigma=(0, 2.0))),\n",
    "    sometimes(iaa.CropAndPad(\n",
    "        percent=(-0.1, 0.2),\n",
    "        pad_mode=ia.ALL,\n",
    "        pad_cval=(0, 255))),\n",
    "    sometimes(iaa.Affine(\n",
    "        scale={\"x\": (0.8, 1.5), \"y\": (0.8, 1.5)},\n",
    "        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "        rotate=(-45, 45),\n",
    "        shear=(-16, 16),\n",
    "        order=[0, 1],\n",
    "        cval=(0, 255),\n",
    "        mode=ia.ALL)),\n",
    "    sometimes(iaa.CoarseDropout(\n",
    "          (0.03, 0.15), size_percent=(0.02, 0.05),\n",
    "          per_channel=0.2)),], random_order = True)\n",
    "    \n",
    "    image = tf.py_function(func=seq.augment_image, inp=[image], Tout=[tf.float32])\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "def make_ds(ds):\n",
    "    ds = ds.map(\n",
    "        process_example_dict, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.cache()\n",
    "    ds = ds.map(augment, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.batch(128)\n",
    "    ds = ds.prefetch(256)\n",
    "    ds = ds.repeat()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to create TF datasets\n",
    "\n",
    "resize = tf.keras.layers.Resizing(\n",
    "    224, 224, interpolation='bilinear', crop_to_aspect_ratio=False)\n",
    "\n",
    "\n",
    "def process_example_dict(example_dict):\n",
    "    image, label = example_dict['image'], example_dict['label']\n",
    "    image = tf.cast(image, tf.float32) / 255.\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def normalize_img(image, label):\n",
    "    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "    image=resize(image)\n",
    "    image = tf.cast(image, tf.float32) / 255.\n",
    "    # image = tf.reshape(image, (-1,))\n",
    "    # tf.print(image.shape)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def make_ds(ds):\n",
    "    ds = ds.map(\n",
    "        process_example_dict, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.cache()\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.batch(128)\n",
    "    ds = ds.prefetch(256)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Conv2D,  MaxPool2D, Flatten, GlobalAveragePooling2D,  BatchNormalization, Layer, Add\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class ResnetBlock(Model):\n",
    "    \"\"\"\n",
    "    A standard resnet block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, down_sample=False):\n",
    "        \"\"\"\n",
    "        channels: same as number of convolution kernels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.__channels = channels\n",
    "        self.__down_sample = down_sample\n",
    "        self.__strides = [2, 1] if down_sample else [1, 1]\n",
    "\n",
    "        KERNEL_SIZE = (3, 3)\n",
    "        # use He initialization, instead of Xavier (a.k.a 'glorot_uniform' in Keras), as suggested in [2]\n",
    "        INIT_SCHEME = \"he_normal\"\n",
    "\n",
    "        self.conv_1 = Conv2D(self.__channels, strides=self.__strides[0],\n",
    "                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
    "        self.bn_1 = BatchNormalization()\n",
    "        self.conv_2 = Conv2D(self.__channels, strides=self.__strides[1],\n",
    "                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
    "        self.bn_2 = BatchNormalization()\n",
    "        self.merge = Add()\n",
    "\n",
    "        if self.__down_sample:\n",
    "            # perform down sampling using stride of 2, according to [1].\n",
    "            self.res_conv = Conv2D(\n",
    "                self.__channels, strides=2, kernel_size=(1, 1), kernel_initializer=INIT_SCHEME, padding=\"same\")\n",
    "            self.res_bn = BatchNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        res = inputs\n",
    "\n",
    "        x = self.conv_1(inputs)\n",
    "        x = self.bn_1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.bn_2(x)\n",
    "\n",
    "        if self.__down_sample:\n",
    "            res = self.res_conv(res)\n",
    "            res = self.res_bn(res)\n",
    "\n",
    "        # if not perform down sample, then add a shortcut directly\n",
    "        x = self.merge([x, res])\n",
    "        out = tf.nn.relu(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet18(Model):\n",
    "\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        \"\"\"\n",
    "            num_classes: number of classes in specific classification task.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv_1 = Conv2D(64, (7, 7), strides=2,\n",
    "                             padding=\"same\", kernel_initializer=\"he_normal\")\n",
    "        self.init_bn = BatchNormalization()\n",
    "        self.pool_2 = MaxPool2D(pool_size=(2, 2), strides=2, padding=\"same\")\n",
    "        self.res_1_1 = ResnetBlock(64)\n",
    "        self.res_1_2 = ResnetBlock(64)\n",
    "        self.res_2_1 = ResnetBlock(128, down_sample=True)\n",
    "        self.res_2_2 = ResnetBlock(128)\n",
    "        self.res_3_1 = ResnetBlock(256, down_sample=True)\n",
    "        self.res_3_2 = ResnetBlock(256)\n",
    "        self.res_4_1 = ResnetBlock(512, down_sample=True)\n",
    "        self.res_4_2 = ResnetBlock(512)\n",
    "        self.avg_pool = GlobalAveragePooling2D()\n",
    "        self.flat = Flatten()\n",
    "        self.fc = Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out = self.conv_1(inputs)\n",
    "        out = self.init_bn(out)\n",
    "        out = tf.nn.relu(out)\n",
    "        out = self.pool_2(out)\n",
    "        for res_block in [self.res_1_1, self.res_1_2, self.res_2_1, self.res_2_2, self.res_3_1, self.res_3_2, self.res_4_1, self.res_4_2]:\n",
    "            out = res_block(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = self.flat(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, GlobalAveragePooling2D, merge, Activation, ZeroPadding2D, Conv2D, MaxPooling2D, BatchNormalization, Concatenate, GlobalMaxPooling2D\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "COMPRESSION = 1.0\n",
    "CHANNEL = 3\n",
    "NUM_FILTER = 128\n",
    "DROPOUT_RATE = 0.\n",
    "N_LAYERS = 3\n",
    "\n",
    "# Dense Block\n",
    "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
    "  temp = input\n",
    "  for _ in range(N_LAYERS):\n",
    "      BatchNorm = BatchNormalization(epsilon=1.1e-5)(temp)\n",
    "      relu = Activation('relu')(BatchNorm)\n",
    "      # kernel_regularizer to regularze kernel weights\n",
    "      # l2 for penallizing weights with large magnitudes\n",
    "      Conv2D_3_3 = Conv2D(int(num_filter*COMPRESSION), (3,3), use_bias=False, padding='same', kernel_regularizer=l2(0.0002))(relu) \n",
    "      if dropout_rate>0:\n",
    "        Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
    "      concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "      temp = concat\n",
    "  return temp\n",
    "\n",
    "\n",
    "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
    "  BatchNorm = BatchNormalization(epsilon=1.1e-5)(input)\n",
    "  relu = Activation('relu')(BatchNorm)\n",
    "  # kernel_regularizer to regularize kernel weights\n",
    "  # l2 for penallizing weights with large magnitudes\n",
    "  Conv2D_BottleNeck = Conv2D(int(num_filter*COMPRESSION), (1,1), use_bias=False ,padding='same',kernel_regularizer=l2(0.0002))(relu)\n",
    "  if dropout_rate>0:\n",
    "    Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "  avg = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(Conv2D_BottleNeck)\n",
    "  return avg\n",
    "\n",
    "\n",
    "def output_layer(input, num_classes):\n",
    "  BatchNorm = BatchNormalization()(input)\n",
    "  relu = Activation('relu')(BatchNorm)\n",
    "  conv2D = Conv2D(num_classes, (1,1), use_bias=False ,kernel_regularizer=l2(0.0002))(relu)\n",
    "  BatchNorm = BatchNormalization()(conv2D)\n",
    "  relu = Activation('relu')(BatchNorm)\n",
    "  GAP = GlobalAveragePooling2D()(relu)\n",
    "  output = Activation('softmax')(GAP)    \n",
    "  return output\n",
    "\n",
    "\n",
    "def DenseNet(num_classes):\n",
    "  input = Input(shape=(None, None, CHANNEL,))\n",
    "  First_Conv2D = Conv2D(NUM_FILTER, (3, 3), use_bias=False, padding='same')(input)\n",
    "  First_Block = add_denseblock(First_Conv2D, NUM_FILTER, DROPOUT_RATE)\n",
    "  First_Transition = add_transition(First_Block, num_filter=256, dropout_rate=DROPOUT_RATE)\n",
    "  Second_Block = add_denseblock(First_Transition, NUM_FILTER, DROPOUT_RATE)\n",
    "  Second_Transition = add_transition(Second_Block, num_filter=320,dropout_rate=DROPOUT_RATE)\n",
    "  Third_Block = add_denseblock(Second_Transition, NUM_FILTER, DROPOUT_RATE)\n",
    "  Third_Transition = add_transition(Third_Block, num_filter=384, dropout_rate=DROPOUT_RATE)\n",
    "  Fourth_Block = add_denseblock(Third_Transition, NUM_FILTER, DROPOUT_RATE)\n",
    "  Fourth_Transition = add_transition(Fourth_Block, num_filter=512, dropout_rate=DROPOUT_RATE)\n",
    "  Fifth_Block = add_denseblock(Fourth_Transition, NUM_FILTER, DROPOUT_RATE)\n",
    "  output = output_layer(Fifth_Block, num_classes)\n",
    "  model = Model(inputs=[input], outputs=[output])\n",
    "  return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset, iterate over it and train, return model and metrics\n",
    "\n",
    "EPS = 1e-3\n",
    "    \n",
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        print(f\"\\nRun training with params {self.params}\")\n",
    "        \n",
    "        # create imgnet dataset and get iterators\n",
    "        tiny_imagenet_builder = TinyImagenetDataset()\n",
    "        tiny_imagenet_builder.download_and_prepare()\n",
    "        ds_train = tiny_imagenet_builder.as_dataset(split=\"train\")\n",
    "        ds_test = tiny_imagenet_builder.as_dataset(split=\"validation\")\n",
    "\n",
    "        # ds_train, ds_test = out = tfds.load(\n",
    "        #     'imagenet_resized/64x64',\n",
    "        #     split=['train', 'validation'],\n",
    "        #     data_dir='/amrith/tensorflow_datasets',\n",
    "        #     shuffle_files=True,\n",
    "        #     as_supervised=True,\n",
    "        #     with_info=False,\n",
    "        # )\n",
    "        \n",
    "        ds_train = make_ds(ds_train)\n",
    "        ds_test = make_ds(ds_test)\n",
    "        self.iter_train = iter(ds_train)\n",
    "        self.iter_test = iter(ds_test)\n",
    "\n",
    "        # define model\n",
    "        # self.model = tf.keras.models.Sequential([\n",
    "        #     tf.keras.layers.RandomFlip(mode='horizontal', input_shape=self.params['input_shape']),\n",
    "        #     tf.keras.layers.RandomTranslation(0.1, 0.1),\n",
    "        #     tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=params['input_shape']),\n",
    "        #     tf.keras.layers.BatchNormalization(fused=True),\n",
    "        #     tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        #     tf.keras.layers.BatchNormalization(fused=True),\n",
    "        #     tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        #     tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        #     tf.keras.layers.BatchNormalization(fused=True),\n",
    "        #     tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        #     tf.keras.layers.BatchNormalization(fused=True),\n",
    "        #     tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        #     tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        #     tf.keras.layers.BatchNormalization(fused=True),\n",
    "        #     tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        #     tf.keras.layers.BatchNormalization(fused=True),\n",
    "        #     tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        #     tf.keras.layers.Flatten(),\n",
    "        #     tf.keras.layers.Dropout(0.2),\n",
    "        #     tf.keras.layers.Dense(1024, activation='relu'),\n",
    "        #     tf.keras.layers.Dropout(0.2),\n",
    "        #     tf.keras.layers.Dense(self.params['n_classes'], activation='softmax')])\n",
    "    \n",
    "    \n",
    "        self.model = DenseNet(self.params['n_classes']) \n",
    "        # ResNet18(self.params['n_classes'])\n",
    "        tf.autograph.experimental.do_not_convert(self.model.build)\n",
    "        self.model.build(input_shape = (None, *(self.params['input_shape'])))\n",
    "        # self.model = tf.keras.applications.resnet50.ResNet50(\n",
    "        #     include_top=True, weights=None, input_shape=self.params['input_shape'])\n",
    "        \n",
    "        print(self.model.summary())\n",
    "\n",
    "        # define optimizer\n",
    "        self.optimizer = tfa.optimizers.SGDW(learning_rate=self.params['lr'], weight_decay=params['weight_decay'], nesterov=True)\n",
    "        \n",
    "        # maintain history\n",
    "        self.history = []\n",
    "        \n",
    "        # get last layer reps\n",
    "        # self.model_last_layer = Model(self.model.input, self.model.layers[-2].output)\n",
    "        \n",
    "        # temperature for platt scaling\n",
    "        self.tau = tf.Variable(tf.ones((1, self.params['n_classes'])))\n",
    "\n",
    "    \n",
    "    def eval_ood(self, X_real, X_fake):\n",
    "        logits = eval_ood_helper(X_real, X_fake, self.tau).numpy()\n",
    "        labels = np.concatenate([np.zeros(128), np.ones(128)])\n",
    "        auc = roc_auc_score(labels, logits)\n",
    "        return auc\n",
    "\n",
    "    \n",
    "    def get_batch_dimension(self, repr): # dimension of the last layer representations from a batch\n",
    "        repr = StandardScaler().fit_transform(repr)\n",
    "        pca = PCA()\n",
    "        pca.fit(repr)\n",
    "        explained_var = pca.explained_variance_\n",
    "        explained_var /= (explained_var.sum() + EPS)\n",
    "        return np.sum([explained_var.cumsum() <= 0.9]) # returns no. of dimensions that account for 90% of variance\n",
    "    \n",
    "    def get_model_weights(self):\n",
    "        params = np.array([])\n",
    "        for layer in t.model.layers:\n",
    "            for wt in layer.trainable_variables: \n",
    "                params = np.concatenate([params, wt.numpy().flatten()], axis=0)\n",
    "        return params\n",
    "    \n",
    "    def set_model_weights(self, param):\n",
    "        idx=0\n",
    "        for layer in self.model.layers:\n",
    "            for wt in layer.trainable_variables: \n",
    "                wt.assign(param[prev:prev+np.prod(wt.shape)].reshape(wt.shape))\n",
    "                idx+=np.prod(wt.shape)\n",
    "                \n",
    "    def evaluate_n_random_batches(self, n=10):\n",
    "        loss = 0.\n",
    "        for _ in range(n):\n",
    "            loss += self.loss_fn(*(self.iter_train), trainable=False)\n",
    "        return loss / n\n",
    "        \n",
    "    def compute_sharpness_metric(self, p=100, delta=0.001):\n",
    "        x_0 = self.get_model_weights() \n",
    "        A = tf.random.normal((x_0.shape[0], p))\n",
    "        proj = tf.linalg.pinv(A) @ x_0\n",
    "        y_min = (tf.math.abs(proj)+1)*delta\n",
    "        y_max = (tf.math.abs(proj)+1)*(-delta)\n",
    "        y_0 = tf.Variable(np.random.zeros(p), trainable=True)\n",
    "        # for LBFS solver, returns func evaluation and gradient\n",
    "        def f(y):\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch()\n",
    "                self.set_model_weights(x_0 + A@y)\n",
    "                loss = - self.evaluate_n_random_batches(n=10)\n",
    "                # we want to maximize the loss hence, the negative sign\n",
    "            return loss, tape.gradient(loss, y)\n",
    "        _, neg_maxf, _ = scipy.optimize.fmin_l_bfgs_b(\n",
    "            f, y_0, bounds=zip(y_min, y_max), maxiter=10)\n",
    "        maxf = -neg_maxf\n",
    "        fx = self.evaluate_n_random_batches(n=10)\n",
    "        sharpness = (maxf - fx) * 100. / (1 + fx)\n",
    "        # reset model weights\n",
    "        self.set_model_weights(x_0)\n",
    "        return sharpness\n",
    "\n",
    "\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        @tf.function\n",
    "        def baseline(X, Y, training):\n",
    "            print(\"Tracing baseline\")\n",
    "            Y_hat = self.model(X, training=training) \n",
    "            ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.argmax(Y_hat, axis=1) == Y, tf.float32)) * 100.\n",
    "            entropy_on_original_point = tf.reduce_mean(-1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat + EPS), axis=1))\n",
    "            return ce_loss, ce_loss, 0., accuracy, entropy_on_original_point\n",
    "\n",
    "        @tf.function\n",
    "        def min_max_cent(X, Y, training, params):\n",
    "            # compute gradient of cross entropy loss wrt X and take a step in +ve direction\n",
    "            # this would try to find a point in the neighborhood of X that maximizes cross entropy\n",
    "            Y_hat = self.model(X, training=training) \n",
    "            ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.argmax(Y_hat, axis=1) == Y, tf.float32)) * 100.\n",
    "            entropy_on_original_point = tf.reduce_mean(-1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat + EPS), axis=1))    \n",
    "            if params['step_size'] > 0.:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    tape.watch(X)\n",
    "                    Y_hat = self.model(X, training=training)\n",
    "                    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "                grads = tape.gradient(loss, X)\n",
    "                grads_norm = tf.norm(tf.reshape(grads, (128, -1)), axis=1)\n",
    "                grads = params['step_size'] * grads / grads_norm[:, None, None, None]\n",
    "                X_perturbed = tf.clip_by_value(X + grads, 0.0, 1.0)\n",
    "            else:\n",
    "                X_perturbed = X\n",
    "            # compute cross entropy at this new point\n",
    "            Y_hat = self.model(X_perturbed, training=training)\n",
    "            loss_adv = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "            return ce_loss + params['lambda'] * loss_adv, ce_loss, loss_adv, accuracy, entropy_on_original_point\n",
    "\n",
    "        @tf.function\n",
    "        def max_min_ent(X, Y, training, params):\n",
    "            # compute grad of entropy wrt X and take a step in negative direction\n",
    "            # this would find a point in the neighborhood of X that would minimize entropy\n",
    "            Y_hat = self.model(X, training=training) \n",
    "            ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.argmax(Y_hat, axis=1) == Y, tf.float32)) * 100.\n",
    "            entropy_on_original_point = tf.reduce_mean(-1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat + EPS), axis=1))    \n",
    "            if params['step_size'] > 0.:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    tape.watch(X)\n",
    "                    Y_hat = self.model(X, training=training)\n",
    "                    exp_neg_entropy = tf.exp(tf.reduce_mean(Y_hat * tf.math.log(Y_hat + EPS), axis=1))\n",
    "                grads = tape.gradient(exp_neg_entropy, X)\n",
    "                grads_norm = tf.norm(tf.reshape(grads, (128, -1)), axis=1)\n",
    "                grads = params['step_size'] * grads / grads_norm[:, None, None, None]      \n",
    "                X_perturbed = tf.clip_by_value(X + grads, 0.0, 1.0)\n",
    "            else:\n",
    "                X_perturbed = X\n",
    "            # compute entropy at this new point and multiply it by -1 (raise to exp. for better grads), since we want to maximize entropy\n",
    "            Y_hat = self.model(X_perturbed, training=training)\n",
    "            entropy = -1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat + EPS), axis=1)\n",
    "            loss_adv = tf.reduce_mean(tf.exp(-1.0 * entropy))\n",
    "            return ce_loss + params['lambda'] * loss_adv, ce_loss, loss_adv, accuracy, entropy_on_original_point\n",
    "\n",
    "        @tf.function\n",
    "        def min_max_KL_unif(X, Y, training, params):\n",
    "            # compute grad of KL(unif, p_\\theta) wrt X and take a step in +ve direction\n",
    "            # this would find a point in the neighborhood of X that would maximize KL\n",
    "            Y_hat = self.model(X, training=training) \n",
    "            ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.argmax(Y_hat, axis=1) == Y, tf.float32)) * 100.\n",
    "            entropy_on_original_point = tf.reduce_mean(-1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat + 1e-3), axis=1)) \n",
    "            if params['step_size'] > 0.:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    tape.watch(X)\n",
    "                    Y_hat = self.model(X, training=training)\n",
    "                    KL_unif = tf.keras.losses.CategoricalCrossentropy(from_logits=False)(\n",
    "                        y_true=tf.ones_like(Y_hat) / params['n_classes'], y_pred=Y_hat)\n",
    "                grads = tape.gradient(KL_unif, X)\n",
    "                grads_norm = tf.norm(tf.reshape(grads, (128, -1)), axis=1)\n",
    "                grads = self.params['step_size'] * grads / grads_norm[:, None, None, None]      \n",
    "                X_perturbed = tf.clip_by_value(X + grads, 0.0, 1.0)\n",
    "            else:\n",
    "                X_perturbed = X\n",
    "            # compute entropy at this new point and multiply it by -1, since we want to maximize entropy\n",
    "            Y_hat = self.model(X_perturbed, training=training)\n",
    "            KL_unif = tf.keras.losses.CategoricalCrossentropy(from_logits=False)(\n",
    "                y_true=tf.ones_like(Y_hat) / params['n_classes'], y_pred=Y_hat)\n",
    "            loss_adv = tf.reduce_mean(KL_unif)\n",
    "            return ce_loss + params['lambda'] * loss_adv, ce_loss, loss_adv, accuracy, entropy_on_original_point\n",
    "\n",
    "        @tf.function\n",
    "        def label_smoothing(X, Y, training, params):\n",
    "            Y_hat = self.model(X, training=training)\n",
    "            ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(y_true=Y, y_pred=Y_hat)\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.argmax(Y_hat, axis=1) == Y, tf.float32)) * 100.\n",
    "            entropy_on_original_point = tf.reduce_mean(-1.0 * tf.reduce_mean(Y_hat * tf.math.log(Y_hat + 1e-3), axis=1)) \n",
    "            Y = tf.one_hot(Y, params['n_classes'])\n",
    "            Y_noisy = Y * (1 - params['label-smoothing-factor']) \n",
    "            Y_noisy += (self.params['label-smoothing-factor'] / tf.cast(params['n_classes'], tf.float32))\n",
    "            noisy_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)(y_true=Y_noisy, y_pred=Y_hat)\n",
    "            return noisy_loss, ce_loss, 0., accuracy, entropy_on_original_point\n",
    "\n",
    "        @tf.function\n",
    "        def get_calibration_metrics(X, Y):\n",
    "            logits = tf.math.log(self.model(X, training=False) + EPS) * tf.repeat(self.tau, X.shape[0], axis=0)\n",
    "            brier = tf.reduce_mean(tfp.stats.brier_score(labels=Y, logits=logits))\n",
    "            ece = tfp.stats.expected_calibration_error(num_bins=20, logits=logits, labels_true=Y)\n",
    "            nll = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(y_true=Y, y_pred=logits)\n",
    "            return brier, ece, nll\n",
    "\n",
    "\n",
    "        @tf.function\n",
    "        def eval_ood_helper(X_real, X_fake):\n",
    "            Y_hat_real = tf.nn.softmax(tf.math.log(self.model(X_real, training=False) + EPS) * tf.repeat(self.tau, X_real.shape[0], axis=0))\n",
    "            entropy_real = -1.0 * tf.reduce_mean(Y_hat_real * tf.math.log(Y_hat_real + EPS), axis=1)\n",
    "            Y_hat_fake = tf.nn.softmax(tf.math.log(self.model(X_fake, training=False) + EPS) * tf.repeat(self.tau, X_fake.shape[0], axis=0))\n",
    "            entropy_fake = -1.0 * tf.reduce_mean(Y_hat_fake * tf.math.log(Y_hat_fake + EPS), axis=1)\n",
    "            return tf.concat([entropy_real, entropy_fake], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "        # define loss function\n",
    "        # computes loss given a model and X, Y\n",
    "        @ tf.function\n",
    "        def loss_fn(X, Y, training, params):\n",
    "\n",
    "            if params['version'] == 'baseline':\n",
    "                return baseline(X, Y, training)\n",
    "            elif params['version'] == 'min-max-cent':\n",
    "                return min_max_cent(X, Y, training, params)\n",
    "            elif params['version'] == 'max-min-ent':\n",
    "                return max_min_ent(X, Y, training, params)\n",
    "            elif params['version'] == 'min-max-KL-unif': \n",
    "                return min_max_KL_unif(X, Y, training, params)\n",
    "            elif params['version'] == 'label-smoothing':\n",
    "                return label_smoothing(X, Y, training, params)\n",
    "            else:\n",
    "                raise ValueError\n",
    "                \n",
    "        # define step function\n",
    "        # computes gradients and applies them\n",
    "        @tf.function\n",
    "        def step_fn(X, Y, params):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss, cent_loss, loss_adv, accuracy, predent = loss_fn(X, Y, True, params)\n",
    "            grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "            return loss, cent_loss, loss_adv, accuracy, predent\n",
    "        \n",
    "        # loop over data n_iters times\n",
    "        for t in tqdm.trange(self.params['n_iters']):\n",
    "            train_loss, train_loss_cent, train_loss_adv, train_acc, train_predent = step_fn(*next(self.iter_train), self.params)    \n",
    "            if t % 20 == 0:\n",
    "                test_loss = []\n",
    "                test_loss_cent = []\n",
    "                test_loss_adv = []\n",
    "                test_acc = []\n",
    "                test_predent = []\n",
    "                for _ in range(10):\n",
    "                    res = loss_fn(*next(self.iter_test), False, self.params)\n",
    "                    test_loss.append(res[0].numpy())\n",
    "                    test_loss_cent.append(res[1].numpy())\n",
    "                    test_loss_adv.append(res[2].numpy())\n",
    "                    test_acc.append(res[3].numpy())\n",
    "                    test_predent.append(res[4].numpy())\n",
    "                # train_dim = self.get_batch_dimension(self.model_last_layer(next(self.iter_train)[0], training=False))\n",
    "                # test_dim = self.get_batch_dimension(self.model_last_layer(next(self.iter_test)[0], training=False))\n",
    "            self.history.append((train_loss.numpy(), np.mean(test_loss), train_acc.numpy(), np.mean(test_acc),\n",
    "                                train_loss_adv.numpy(), np.mean(test_loss_adv), train_predent.numpy(), np.mean(test_predent)))\n",
    "                                # train_dim, test_dim))\n",
    "            \n",
    "            if t % 1000 == 0:\n",
    "                tf.print(\"Tr Total:\", train_loss, \"Tr CE:\", train_loss_cent, \"Tr Adv:\", train_loss_adv, \"Tr Acc:\", train_acc, \"Test Acc\", self.history[-1][3])\n",
    "            \n",
    "            if ('lambda_schedule' in self.params) and ((t+1) % self.params['lambda_schedule']['frequency'] == 0):\n",
    "                self.params['lambda'] *= self.params['lambda_schedule']['factor']\n",
    "                \n",
    "            if ('lr_schedule' in self.params) and ((t+1) % self.params['lr_schedule']['frequency'] == 0):\n",
    "                self.optimizer.lr.assign(self.optimizer.lr * self.params['lr_schedule']['factor'])\n",
    "                \n",
    "\n",
    "        self.history = np.array(self.history)\n",
    "\n",
    "        \n",
    "# post hoc platt scaling\n",
    "def calibrate_model(trainer):\n",
    "    # loop over data n_iters times\n",
    "    tau_optimizer = tf.keras.optimizers.Adam(learning_rate=trainer.params['lr-calibrator']*10.)\n",
    "    for t in tqdm.trange(trainer.params['n_iters']//2):\n",
    "        X, Y = next(trainer.iter_train)\n",
    "        Y_pred_logits = tf.math.log(trainer.model(X, training=False) + EPS)\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(trainer.tau)\n",
    "            Y_pred_logits *= tf.repeat(trainer.tau, Y_pred_logits.shape[0], axis=0)\n",
    "            loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(y_true=Y, y_pred=Y_pred_logits)\n",
    "        grad_tau = tape.gradient(loss, trainer.tau)\n",
    "        grad_tau = tf.clip_by_norm(grad_tau, 0.1)\n",
    "        tau_optimizer.apply_gradients([(grad_tau, trainer.tau)])\n",
    "        if t % 1000 == 0:\n",
    "            print(f\"Calibration Loss: {loss}, {trainer.tau}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline\n",
    "params = {\n",
    "    'input_shape': (64, 64, 3),\n",
    "    'n_classes': 200,\n",
    "    'lambda': 0.0,\n",
    "    'lr': 5e-4,\n",
    "    'lr-calibrator': 1e-4,\n",
    "    'step_size': 0.,\n",
    "    'n_iters': 10000,\n",
    "    'version': 'baseline',\n",
    "    'weight_decay': 0.0001,\n",
    "    'lr_schedule':{\n",
    "        'frequency':8000,\n",
    "        'factor': 1.\n",
    "    },\n",
    "    'label-smoothing-factor': 0.\n",
    "}\n",
    "baseline_trainer = Trainer(params)\n",
    "baseline_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max-min-ent\n",
    "params = {\n",
    "    'input_shape': (64, 64, 3),\n",
    "    'n_classes': 200,\n",
    "    'lambda': 5.,\n",
    "    'lr': 5e-4,\n",
    "    'lr-calibrator': 1e-4,\n",
    "    'step_size': 0.2,\n",
    "    'n_iters': 10000,\n",
    "    'version': 'max-min-ent',\n",
    "    'weight_decay': 0.0001,\n",
    "    'lr_schedule':{\n",
    "        'frequency':8000,\n",
    "        'factor': 0.1\n",
    "    },\n",
    "    'lambda_schedule': {\n",
    "        'frequency': 500,\n",
    "        'factor': 1.\n",
    "    }\n",
    "}\n",
    "max_min_ent_trainer = Trainer(params)\n",
    "max_min_ent_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-max-KL-unif\n",
    "params = {\n",
    "    'input_shape': (64, 64, 3),\n",
    "    'n_classes': 200,\n",
    "    'lambda': 0.1,\n",
    "    'lr': 5e-4,\n",
    "    'lr-calibrator': 1e-4,\n",
    "    'step_size': 0.2,\n",
    "    'n_iters': 10000,\n",
    "    'version': 'min-max-KL-unif',\n",
    "    'weight_decay': 0.0001,\n",
    "    'lr_schedule':{\n",
    "        'frequency':8000,\n",
    "        'factor': 0.1\n",
    "    },\n",
    "    'lambda_schedule': {\n",
    "        'frequency': 500,\n",
    "        'factor': 1.\n",
    "    }\n",
    "}\n",
    "min_max_KL_unif_trainer = Trainer(params)\n",
    "min_max_KL_unif_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute ID metrics\n",
    "ID_results = [\n",
    "    [np.mean(baseline_trainer.history[:, 2][-20:]), np.mean(baseline_trainer.history[:, 3][-20:])],\n",
    "    [np.mean(max_min_ent_trainer.history[:, 2][-20:]), np.mean(max_min_ent_trainer.history[:, 3][-20:])],    \n",
    "    # [np.mean(min_max_KL_unif_trainer.history[:, 2][-20:]), np.mean(min_max_KL_unif_trainer.history[:, 3][-20:])],\n",
    "]\n",
    "plot_ID_metrics(ID_results, tags = ['baseline', 'max_min_ent',])\n",
    "# 'min_max_KL_unif'])\n",
    "plot_training_metrics(baseline_trainer, min_max_KL_unif_trainer, ['baseline', 'max_min_ent',])\n",
    "# 'min_max_KL_unif'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-max-KL-unif\n",
    "params = {\n",
    "    'input_shape': (64, 64, 3),\n",
    "    'n_classes': 200,\n",
    "    'lambda': 0.2,\n",
    "    'lr': 5e-4,\n",
    "    'lr-calibrator': 1e-4,\n",
    "    'step_size': 0.2,\n",
    "    'n_iters': 20000,\n",
    "    'version': 'min-max-KL-unif',\n",
    "    'weight_decay': 0.0001,\n",
    "    'lr_schedule':{\n",
    "        'frequency':16000,\n",
    "        'factor': 0.1\n",
    "    },\n",
    "    'lambda_schedule': {\n",
    "        'frequency': 500,\n",
    "        'factor': 1.\n",
    "    }\n",
    "}\n",
    "min_max_KL_unif_trainer2 = Trainer(params)\n",
    "min_max_KL_unif_trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute ID metrics\n",
    "ID_results = [\n",
    "    [np.mean(baseline_trainer.history[:, 2][-20:]), np.mean(baseline_trainer.history[:, 3][-20:])],\n",
    "    [np.mean(min_max_KL_unif_trainer2.history[:, 2][-20:]), np.mean(min_max_KL_unif_trainer2.history[:, 3][-20:])],\n",
    "]\n",
    "plot_ID_metrics(ID_results, tags = ['baseline', 'min_max_KL_unif'])\n",
    "plot_training_metrics(baseline_trainer, min_max_KL_unif_trainer2, ['baseline', 'min_max_KL_unif'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting utils\n",
    "def plot_training_metrics(baseline_trainer, our_trainer, tags=['baseline', '']):\n",
    "    \n",
    "    trainer_vec = [baseline_trainer, our_trainer]\n",
    "    \n",
    "    c_vec = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    plt.figure(figsize=(20, 3))\n",
    "\n",
    "    # total loss\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.title(\"Train/Test total loss\")\n",
    "    for idx, (c, trainer) in enumerate(zip(c_vec, trainer_vec)):\n",
    "        name = tags[idx]\n",
    "        plt.plot(gaussian_filter1d(trainer.history[:, 0], 100), '-o', c=c, label='%s - train' % name, markevery=1000, markersize=10)\n",
    "        plt.plot(gaussian_filter1d(trainer.history[:, 1], 100), '-^', c=c, label='%s - val' % name, markevery=1000, markersize=10)\n",
    "    plt.grid()\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('total loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "    # accuracy\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.title(\"Train/Test accuracy\")\n",
    "    for idx, (c, trainer) in enumerate(zip(c_vec, trainer_vec)):\n",
    "        name = tags[idx]\n",
    "        plt.plot(gaussian_filter1d(trainer.history[:, 2], 100), '-o', c=c, label='%s - train' % name, markevery=1000, markersize=10)\n",
    "        plt.plot(gaussian_filter1d(trainer.history[:, 3], 100), '-^', c=c, label='%s - val' % name, markevery=1000, markersize=10)\n",
    "    plt.grid()\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "\n",
    "    # entropy\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.title(\"Train/Test predictive entropy\")\n",
    "    for idx, (c, trainer) in enumerate(zip(c_vec, trainer_vec)):\n",
    "        name = tags[idx]\n",
    "        plt.plot(np.abs(gaussian_filter1d(trainer.history[:, 6], 100)), '-o', c=c, label='%s - train' % name, markevery=1000, markersize=10)\n",
    "        plt.plot(np.abs(gaussian_filter1d(trainer.history[:, 7], 100)), '-^', c=c, label='%s - val' % name, markevery=1000, markersize=10)\n",
    "    plt.grid()\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('Entropy')\n",
    "    \n",
    "    # dimensionality\n",
    "    # plt.subplot(1, 4, 4)\n",
    "    # plt.title(\"Train/Test last-layer-rep dim\")\n",
    "    # for idx, (c, trainer) in enumerate(zip(c_vec, trainer_vec)):\n",
    "    #     name = tags[idx]\n",
    "    #     plt.plot(gaussian_filter1d(trainer.history[:, 8], 100), '-o', c=c, label='%s - train' % name, markevery=1000, markersize=10)\n",
    "    #     plt.plot(gaussian_filter1d(trainer.history[:, 9], 100), '-^', c=c, label='%s - val' % name, markevery=1000, markersize=10)\n",
    "    # plt.grid()\n",
    "    # plt.xlabel('iterations')\n",
    "    # plt.ylabel('# dims that capture 90% of feature var')\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "def plot_OOD_metrics(results, tags):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for (j, title) in enumerate(['Accuracy OOD $(\\\\uparrow)$', 'OOD AUC $(\\\\uparrow)$', 'Brier $(\\\\downarrow)$', 'ECE  $(\\\\downarrow)$', 'NLL  $(\\\\downarrow)$']):\n",
    "        plt.subplot(2, 3, j + 1)\n",
    "        plt.title(title)\n",
    "        for idx in range(len(tags)):\n",
    "            x = np.arange(len(corruption_type_list))\n",
    "            y = [results[(ctype, idx)][j] for ctype in corruption_type_list]\n",
    "            width = 0.1\n",
    "            offset = width\n",
    "            plt.bar(x + width * (idx + 1) - offset, y, width=width)\n",
    "        plt.xticks(np.arange(len(corruption_type_list)) + 2*width, corruption_type_list, rotation=90)\n",
    "        plt.grid()\n",
    "\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.title('legend')\n",
    "    for idx in range(len(tags)):\n",
    "        plt.scatter(0., 0., label=f'{tags[idx]}')\n",
    "    plt.legend(fontsize=8, loc='lower right')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_ID_metrics(results, tags):\n",
    "    plt.figure(figsize=(5, 6))\n",
    "    for (j, title) in enumerate(['Train Accuracy ID $(\\\\uparrow)$', 'Test Accuracy ID $(\\\\uparrow)$']):\n",
    "        plt.subplot(1, 2, j + 1)\n",
    "        plt.title(title)\n",
    "        width = 1.\n",
    "        offset = width    \n",
    "        for idx in range(len(tags)):\n",
    "            plt.bar(width * (idx + 1), results[idx][j], width=width)\n",
    "        plt.xticks(np.arange(len(tags)) + width, tags, rotation=90)\n",
    "        plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class Test(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.model = tf.keras.layers.Dense(1)\n",
    "    self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    \n",
    "  @tf.function\n",
    "  def step(self, X, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = tf.reduce_mean((self.model(X) - y) ** 2)\n",
    "    grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "    self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "    return loss\n",
    "    \n",
    "t = Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.random.normal((2, 10))\n",
    "y = tf.random.normal((2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    print(t.step(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "name": "predictive entropy.ipynb",
   "provenance": [
    {
     "file_id": "1JQbpV3_b5nJeSwR-vLJ-t9xoCu33COGH",
     "timestamp": 1630008033655
    },
    {
     "file_id": "1l0iWRfgfMvCsvz6Bqu-RcHKkeHEF48qb",
     "timestamp": 1628698030421
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
